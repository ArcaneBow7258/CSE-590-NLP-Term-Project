{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Summarization with Deep Reinforcement Learning\n",
    "Paper[1] is linked: [Improving Code Summarization with Deep Reinforcement Learning](https://arxiv.org/abs/1811.07234)  \n",
    "We're attempting to add concepts from Paper[2] [Unsupervised Translation of Programming Languages](https://arxiv.org/pdf/2006.03511)  \n",
    "How? Not sure...  \n",
    "I'm thinking of using their masking technique to finetune the product of Paper[1]. Essentially, with a given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cache_dir = 'F:\\\\.cache\\\\huggingface\\\\'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "import code_search_net\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the below line instead of you don't want to load locally.\n",
    "# BUt I havne't figured out to only choose certain datasets, I think its the uploader's fault\n",
    "# Total is like 10+GB's\n",
    "# d = datasets.load_dataset('code-search-net/code_search_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# What language you used for the data, otherwise default to all\n",
    "dl_manager = datasets.DownloadManager('python')\n",
    "csn = code_search_net.CodeSearchNet(config_name='python')\n",
    "# You can remove the loc variable. My c:drive has no space so I did this instead.\n",
    "generators = csn._split_generators(dl_manager, loc = 'F:\\\\.cache\\\\huggingface\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for i, data in csn._generate_examples(generators[0].gen_kwargs['filepaths'][:1]):\n",
    "    raw_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of interest:\n",
    "# func_code_tokens\n",
    "# func_documentation_tokens\n",
    "# possibly func_path_in_repository?\n",
    "print(len(raw_data))\n",
    "raw_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Inputs\n",
    "\"To convert code into sequential text, we tokenize the code by `{. , ” ’ : ; ) ( ! (space)}`, which  has been used in [8].  \n",
    "We tokenize the comment by {(space)}\" - Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Vocab Creation\n",
    "- Reparse func_code_string into tokens\n",
    "- Might respase func_documaetnation_string\n",
    "- lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab / Max Sentence Length\n",
      "202545 4091  |  21314 564\n"
     ]
    }
   ],
   "source": [
    "vocab_code = {'SOS' : 0} # Is this necessary?\n",
    "max_code = 0\n",
    "i = 1\n",
    "vocab_comm = {'SOS' : 0}\n",
    "max_comm = 0\n",
    "j = 1\n",
    "for f in raw_data:\n",
    "    # I think we will need to preprocess out own toeksn btu for right now prrof of concenpt:\n",
    "    for code_token in f['func_code_tokens']:\n",
    "        if code_token not in vocab_code.keys():\n",
    "            vocab_code[code_token] = i\n",
    "            i += 1\n",
    "    max_code = max(max_code, len(f['func_code_tokens']))\n",
    "    for comm_token in f['func_documentation_tokens']:\n",
    "        if comm_token not in vocab_comm.keys():\n",
    "            vocab_comm[comm_token] = j\n",
    "            j += 1\n",
    "    max_comm = max(max_comm, len(f['func_documentation_tokens']))\n",
    "print('Vocab / Max Sentence Length')\n",
    "print(len(vocab_code), max_code, ' | ', len(vocab_comm), max_comm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SOS': 0,\n",
       " 'def': 1,\n",
       " 'train': 2,\n",
       " '(': 3,\n",
       " 'train_dir': 4,\n",
       " ',': 5,\n",
       " 'model_save_path': 6,\n",
       " '=': 7,\n",
       " 'None': 8,\n",
       " 'n_neighbors': 9,\n",
       " 'knn_algo': 10,\n",
       " \"'ball_tree'\": 11,\n",
       " 'verbose': 12,\n",
       " 'False': 13,\n",
       " ')': 14,\n",
       " ':': 15,\n",
       " 'X': 16,\n",
       " '[': 17,\n",
       " ']': 18,\n",
       " 'y': 19,\n",
       " '# Loop through each person in the training set': 20,\n",
       " 'for': 21,\n",
       " 'class_dir': 22,\n",
       " 'in': 23,\n",
       " 'os': 24,\n",
       " '.': 25,\n",
       " 'listdir': 26,\n",
       " 'if': 27,\n",
       " 'not': 28,\n",
       " 'path': 29,\n",
       " 'isdir': 30,\n",
       " 'join': 31,\n",
       " 'continue': 32,\n",
       " '# Loop through each training image for the current person': 33,\n",
       " 'img_path': 34,\n",
       " 'image_files_in_folder': 35,\n",
       " 'image': 36,\n",
       " 'face_recognition': 37,\n",
       " 'load_image_file': 38,\n",
       " 'face_bounding_boxes': 39,\n",
       " 'face_locations': 40,\n",
       " 'len': 41,\n",
       " '!=': 42,\n",
       " '1': 43,\n",
       " '# If there are no people (or too many people) in a training image, skip the image.': 44,\n",
       " 'print': 45,\n",
       " '\"Image {} not suitable for training: {}\"': 46,\n",
       " 'format': 47,\n",
       " '\"Didn\\'t find a face\"': 48,\n",
       " '<': 49,\n",
       " 'else': 50,\n",
       " '\"Found more than one face\"': 51,\n",
       " '# Add face encoding for current image to the training set': 52,\n",
       " 'append': 53,\n",
       " 'face_encodings': 54,\n",
       " 'known_face_locations': 55,\n",
       " '0': 56,\n",
       " '# Determine how many neighbors to use for weighting in the KNN classifier': 57,\n",
       " 'is': 58,\n",
       " 'int': 59,\n",
       " 'round': 60,\n",
       " 'math': 61,\n",
       " 'sqrt': 62,\n",
       " '\"Chose n_neighbors automatically:\"': 63,\n",
       " '# Create and train the KNN classifier': 64,\n",
       " 'knn_clf': 65,\n",
       " 'neighbors': 66,\n",
       " 'KNeighborsClassifier': 67,\n",
       " 'algorithm': 68,\n",
       " 'weights': 69,\n",
       " \"'distance'\": 70,\n",
       " 'fit': 71,\n",
       " '# Save the trained KNN classifier': 72,\n",
       " 'with': 73,\n",
       " 'open': 74,\n",
       " \"'wb'\": 75,\n",
       " 'as': 76,\n",
       " 'f': 77,\n",
       " 'pickle': 78,\n",
       " 'dump': 79,\n",
       " 'return': 80,\n",
       " 'predict': 81,\n",
       " 'X_img_path': 82,\n",
       " 'model_path': 83,\n",
       " 'distance_threshold': 84,\n",
       " '0.6': 85,\n",
       " 'isfile': 86,\n",
       " 'or': 87,\n",
       " 'splitext': 88,\n",
       " 'ALLOWED_EXTENSIONS': 89,\n",
       " 'raise': 90,\n",
       " 'Exception': 91,\n",
       " '\"Invalid image path: {}\"': 92,\n",
       " 'and': 93,\n",
       " '\"Must supply knn classifier either thourgh knn_clf or model_path\"': 94,\n",
       " '# Load a trained KNN model (if one was passed in)': 95,\n",
       " \"'rb'\": 96,\n",
       " 'load': 97,\n",
       " '# Load image file and find face locations': 98,\n",
       " 'X_img': 99,\n",
       " 'X_face_locations': 100,\n",
       " '# If no faces are found in the image, return an empty result.': 101,\n",
       " '==': 102,\n",
       " '# Find encodings for faces in the test iamge': 103,\n",
       " 'faces_encodings': 104,\n",
       " '# Use the KNN model to find the best matches for the test face': 105,\n",
       " 'closest_distances': 106,\n",
       " 'kneighbors': 107,\n",
       " 'are_matches': 108,\n",
       " 'i': 109,\n",
       " '<=': 110,\n",
       " 'range': 111,\n",
       " \"# Predict classes and remove classifications that aren't within the threshold\": 112,\n",
       " 'pred': 113,\n",
       " 'loc': 114,\n",
       " 'rec': 115,\n",
       " '\"unknown\"': 116,\n",
       " 'zip': 117,\n",
       " 'show_prediction_labels_on_image': 118,\n",
       " 'predictions': 119,\n",
       " 'pil_image': 120,\n",
       " 'Image': 121,\n",
       " 'convert': 122,\n",
       " '\"RGB\"': 123,\n",
       " 'draw': 124,\n",
       " 'ImageDraw': 125,\n",
       " 'Draw': 126,\n",
       " 'name': 127,\n",
       " 'top': 128,\n",
       " 'right': 129,\n",
       " 'bottom': 130,\n",
       " 'left': 131,\n",
       " '# Draw a box around the face using the Pillow module': 132,\n",
       " 'rectangle': 133,\n",
       " 'outline': 134,\n",
       " '255': 135,\n",
       " \"# There's a bug in Pillow where it blows up with non-UTF-8 text\": 136,\n",
       " '# when using the default bitmap font': 137,\n",
       " 'encode': 138,\n",
       " '\"UTF-8\"': 139,\n",
       " '# Draw a label with a name below the face': 140,\n",
       " 'text_width': 141,\n",
       " 'text_height': 142,\n",
       " 'textsize': 143,\n",
       " '-': 144,\n",
       " '10': 145,\n",
       " 'fill': 146,\n",
       " 'text': 147,\n",
       " '+': 148,\n",
       " '6': 149,\n",
       " '5': 150,\n",
       " '# Remove the drawing library from memory as per the Pillow docs': 151,\n",
       " 'del': 152,\n",
       " '# Display the resulting image': 153,\n",
       " 'show': 154,\n",
       " '_rect_to_css': 155,\n",
       " 'rect': 156,\n",
       " '_trim_css_to_bounds': 157,\n",
       " 'css': 158,\n",
       " 'image_shape': 159,\n",
       " 'max': 160,\n",
       " 'min': 161,\n",
       " '2': 162,\n",
       " '3': 163,\n",
       " 'face_distance': 164,\n",
       " 'face_to_compare': 165,\n",
       " 'np': 166,\n",
       " 'empty': 167,\n",
       " 'linalg': 168,\n",
       " 'norm': 169,\n",
       " 'axis': 170,\n",
       " 'file': 171,\n",
       " 'mode': 172,\n",
       " \"'RGB'\": 173,\n",
       " 'im': 174,\n",
       " 'PIL': 175,\n",
       " 'array': 176,\n",
       " '_raw_face_locations': 177,\n",
       " 'img': 178,\n",
       " 'number_of_times_to_upsample': 179,\n",
       " 'model': 180,\n",
       " '\"hog\"': 181,\n",
       " '\"cnn\"': 182,\n",
       " 'cnn_face_detector': 183,\n",
       " 'face_detector': 184,\n",
       " 'face': 185,\n",
       " 'shape': 186,\n",
       " 'batch_face_locations': 187,\n",
       " 'images': 188,\n",
       " 'batch_size': 189,\n",
       " '128': 190,\n",
       " 'convert_cnn_detections_to_css': 191,\n",
       " 'detections': 192,\n",
       " 'raw_detections_batched': 193,\n",
       " '_raw_face_locations_batched': 194,\n",
       " 'list': 195,\n",
       " 'map': 196,\n",
       " 'face_landmarks': 197,\n",
       " 'face_image': 198,\n",
       " '\"large\"': 199,\n",
       " 'landmarks': 200,\n",
       " '_raw_face_landmarks': 201,\n",
       " 'landmarks_as_tuples': 202,\n",
       " 'p': 203,\n",
       " 'x': 204,\n",
       " 'landmark': 205,\n",
       " 'parts': 206,\n",
       " '# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png': 207,\n",
       " \"'large'\": 208,\n",
       " '{': 209,\n",
       " '\"chin\"': 210,\n",
       " 'points': 211,\n",
       " '17': 212,\n",
       " '\"left_eyebrow\"': 213,\n",
       " '22': 214,\n",
       " '\"right_eyebrow\"': 215,\n",
       " '27': 216,\n",
       " '\"nose_bridge\"': 217,\n",
       " '31': 218,\n",
       " '\"nose_tip\"': 219,\n",
       " '36': 220,\n",
       " '\"left_eye\"': 221,\n",
       " '42': 222,\n",
       " '\"right_eye\"': 223,\n",
       " '48': 224,\n",
       " '\"top_lip\"': 225,\n",
       " '55': 226,\n",
       " '64': 227,\n",
       " '63': 228,\n",
       " '62': 229,\n",
       " '61': 230,\n",
       " '60': 231,\n",
       " '\"bottom_lip\"': 232,\n",
       " '54': 233,\n",
       " '67': 234,\n",
       " '66': 235,\n",
       " '65': 236,\n",
       " '}': 237,\n",
       " 'elif': 238,\n",
       " \"'small'\": 239,\n",
       " '4': 240,\n",
       " 'ValueError': 241,\n",
       " '\"Invalid landmarks model type. Supported models are [\\'small\\', \\'large\\'].\"': 242,\n",
       " 'num_jitters': 243,\n",
       " 'raw_landmarks': 244,\n",
       " '\"small\"': 245,\n",
       " 'face_encoder': 246,\n",
       " 'compute_face_descriptor': 247,\n",
       " 'raw_landmark_set': 248,\n",
       " '_parse_datatype_string': 249,\n",
       " 's': 250,\n",
       " 'sc': 251,\n",
       " 'SparkContext': 252,\n",
       " '_active_spark_context': 253,\n",
       " 'from_ddl_schema': 254,\n",
       " 'type_str': 255,\n",
       " '_parse_datatype_json_string': 256,\n",
       " '_jvm': 257,\n",
       " 'org': 258,\n",
       " 'apache': 259,\n",
       " 'spark': 260,\n",
       " 'sql': 261,\n",
       " 'types': 262,\n",
       " 'StructType': 263,\n",
       " 'fromDDL': 264,\n",
       " 'json': 265,\n",
       " 'from_ddl_datatype': 266,\n",
       " 'api': 267,\n",
       " 'python': 268,\n",
       " 'PythonSQLUtils': 269,\n",
       " 'parseDataType': 270,\n",
       " 'try': 271,\n",
       " '# DDL format, \"fieldname datatype, fieldname datatype\".': 272,\n",
       " 'except': 273,\n",
       " 'e': 274,\n",
       " '# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.': 275,\n",
       " '# For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case.': 276,\n",
       " '\"struct<%s>\"': 277,\n",
       " '%': 278,\n",
       " 'strip': 279,\n",
       " '_int_size_to_type': 280,\n",
       " 'size': 281,\n",
       " '8': 282,\n",
       " 'ByteType': 283,\n",
       " '16': 284,\n",
       " 'ShortType': 285,\n",
       " '32': 286,\n",
       " 'IntegerType': 287,\n",
       " 'LongType': 288,\n",
       " '_infer_type': 289,\n",
       " 'obj': 290,\n",
       " 'NullType': 291,\n",
       " 'hasattr': 292,\n",
       " \"'__UDT__'\": 293,\n",
       " '__UDT__': 294,\n",
       " 'dataType': 295,\n",
       " '_type_mappings': 296,\n",
       " 'get': 297,\n",
       " 'type': 298,\n",
       " 'DecimalType': 299,\n",
       " '# the precision and scale of `obj` may be different from row to row.': 300,\n",
       " '38': 301,\n",
       " '18': 302,\n",
       " 'isinstance': 303,\n",
       " 'dict': 304,\n",
       " 'key': 305,\n",
       " 'value': 306,\n",
       " 'items': 307,\n",
       " 'MapType': 308,\n",
       " 'True': 309,\n",
       " 'v': 310,\n",
       " 'ArrayType': 311,\n",
       " 'typecode': 312,\n",
       " '_array_type_mappings': 313,\n",
       " 'TypeError': 314,\n",
       " '\"not supported type: array(%s)\"': 315,\n",
       " '_infer_schema': 316,\n",
       " '\"not supported type: %s\"': 317,\n",
       " 'row': 318,\n",
       " 'names': 319,\n",
       " 'sorted': 320,\n",
       " 'tuple': 321,\n",
       " '\"__fields__\"': 322,\n",
       " '# Row': 323,\n",
       " '__fields__': 324,\n",
       " '\"_fields\"': 325,\n",
       " '# namedtuple': 326,\n",
       " '_fields': 327,\n",
       " \"'_%d'\": 328,\n",
       " 'extend': 329,\n",
       " '\"__dict__\"': 330,\n",
       " '# object': 331,\n",
       " '__dict__': 332,\n",
       " '\"Can not infer schema for type: %s\"': 333,\n",
       " 'fields': 334,\n",
       " 'StructField': 335,\n",
       " 'k': 336,\n",
       " '_has_nulltype': 337,\n",
       " 'dt': 338,\n",
       " 'any': 339,\n",
       " 'elementType': 340,\n",
       " 'keyType': 341,\n",
       " 'valueType': 342,\n",
       " '_create_converter': 343,\n",
       " '_need_converter': 344,\n",
       " 'lambda': 345,\n",
       " 'conv': 346,\n",
       " 'kconv': 347,\n",
       " 'vconv': 348,\n",
       " '# dataType must be StructType': 349,\n",
       " 'converters': 350,\n",
       " 'convert_fields': 351,\n",
       " 'convert_struct': 352,\n",
       " 'd': 353,\n",
       " '\"Unexpected obj type: %s\"': 354,\n",
       " '_make_type_verifier': 355,\n",
       " 'nullable': 356,\n",
       " 'new_msg': 357,\n",
       " 'msg': 358,\n",
       " 'new_name': 359,\n",
       " 'n': 360,\n",
       " '\"field %s\"': 361,\n",
       " '\"%s: %s\"': 362,\n",
       " '\"field %s in %s\"': 363,\n",
       " 'verify_nullability': 364,\n",
       " '\"This field is not nullable, but got None\"': 365,\n",
       " '_type': 366,\n",
       " 'assert_acceptable_types': 367,\n",
       " 'assert': 368,\n",
       " '_acceptable_types': 369,\n",
       " '\"unknown datatype: %s for object %r\"': 370,\n",
       " 'verify_acceptable_types': 371,\n",
       " '# subclass of them can not be fromInternal in JVM': 372,\n",
       " '\"%s can not accept object %r in type %s\"': 373,\n",
       " 'StringType': 374,\n",
       " '# StringType can work with any types': 375,\n",
       " 'verify_value': 376,\n",
       " '_': 377,\n",
       " 'UserDefinedType': 378,\n",
       " 'verifier': 379,\n",
       " 'sqlType': 380,\n",
       " 'verify_udf': 381,\n",
       " '\"%r is not an instance of type %r\"': 382,\n",
       " 'toInternal': 383,\n",
       " 'verify_byte': 384,\n",
       " '>': 385,\n",
       " '127': 386,\n",
       " '\"object of ByteType out of range, got: %s\"': 387,\n",
       " 'verify_short': 388,\n",
       " '32768': 389,\n",
       " '32767': 390,\n",
       " '\"object of ShortType out of range, got: %s\"': 391,\n",
       " 'verify_integer': 392,\n",
       " '2147483648': 393,\n",
       " '2147483647': 394,\n",
       " '\"object of IntegerType out of range, got: %s\"': 395,\n",
       " 'element_verifier': 396,\n",
       " 'containsNull': 397,\n",
       " '\"element in array %s\"': 398,\n",
       " 'verify_array': 399,\n",
       " 'key_verifier': 400,\n",
       " '\"key of map %s\"': 401,\n",
       " 'value_verifier': 402,\n",
       " 'valueContainsNull': 403,\n",
       " '\"value of map %s\"': 404,\n",
       " 'verify_map': 405,\n",
       " 'verifiers': 406,\n",
       " 'verify_struct': 407,\n",
       " 'Row': 408,\n",
       " 'getattr': 409,\n",
       " '\"__from_dict__\"': 410,\n",
       " '# the order in obj could be different than dataType.fields': 411,\n",
       " '\"Length of object (%d) does not match with \"': 412,\n",
       " '\"length of fields (%d)\"': 413,\n",
       " '\"StructType can not accept object %r in type %s\"': 414,\n",
       " 'verify_default': 415,\n",
       " 'verify': 416,\n",
       " 'to_arrow_type': 417,\n",
       " 'import': 418,\n",
       " 'pyarrow': 419,\n",
       " 'pa': 420,\n",
       " 'BooleanType': 421,\n",
       " 'arrow_type': 422,\n",
       " 'bool_': 423,\n",
       " 'int8': 424,\n",
       " 'int16': 425,\n",
       " 'int32': 426,\n",
       " 'int64': 427,\n",
       " 'FloatType': 428,\n",
       " 'float32': 429,\n",
       " 'DoubleType': 430,\n",
       " 'float64': 431,\n",
       " 'decimal128': 432,\n",
       " 'precision': 433,\n",
       " 'scale': 434,\n",
       " 'string': 435,\n",
       " 'BinaryType': 436,\n",
       " 'binary': 437,\n",
       " 'DateType': 438,\n",
       " 'date32': 439,\n",
       " 'TimestampType': 440,\n",
       " '# Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read': 441,\n",
       " 'timestamp': 442,\n",
       " \"'us'\": 443,\n",
       " 'tz': 444,\n",
       " \"'UTC'\": 445,\n",
       " '\"Unsupported type in conversion to Arrow: \"': 446,\n",
       " 'str': 447,\n",
       " 'list_': 448,\n",
       " 'field': 449,\n",
       " '\"Nested StructType not supported in conversion to Arrow\"': 450,\n",
       " 'struct': 451,\n",
       " 'to_arrow_schema': 452,\n",
       " 'schema': 453,\n",
       " 'from_arrow_type': 454,\n",
       " 'at': 455,\n",
       " 'is_boolean': 456,\n",
       " 'spark_type': 457,\n",
       " 'is_int8': 458,\n",
       " 'is_int16': 459,\n",
       " 'is_int32': 460,\n",
       " 'is_int64': 461,\n",
       " 'is_float32': 462,\n",
       " 'is_float64': 463,\n",
       " 'is_decimal': 464,\n",
       " 'is_string': 465,\n",
       " 'is_binary': 466,\n",
       " 'is_date32': 467,\n",
       " 'is_timestamp': 468,\n",
       " 'is_list': 469,\n",
       " 'value_type': 470,\n",
       " '\"Unsupported type in conversion from Arrow: \"': 471,\n",
       " 'is_struct': 472,\n",
       " '\"Nested StructType not supported in conversion from Arrow: \"': 473,\n",
       " 'from_arrow_schema': 474,\n",
       " 'arrow_schema': 475,\n",
       " '_check_series_localize_timestamps': 476,\n",
       " 'timezone': 477,\n",
       " 'from': 478,\n",
       " 'pyspark': 479,\n",
       " 'utils': 480,\n",
       " 'require_minimum_pandas_version': 481,\n",
       " 'pandas': 482,\n",
       " 'is_datetime64tz_dtype': 483,\n",
       " '_get_local_timezone': 484,\n",
       " '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?': 485,\n",
       " 'dtype': 486,\n",
       " 'tz_convert': 487,\n",
       " 'tz_localize': 488,\n",
       " '_check_dataframe_localize_timestamps': 489,\n",
       " 'pdf': 490,\n",
       " 'column': 491,\n",
       " 'series': 492,\n",
       " 'iteritems': 493,\n",
       " '_check_series_convert_timestamps_internal': 494,\n",
       " 'is_datetime64_dtype': 495,\n",
       " '# When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive': 496,\n",
       " '# timestamp is during the hour when the clock is adjusted backward during due to': 497,\n",
       " '# daylight saving time (dst).': 498,\n",
       " '# E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to': 499,\n",
       " '# 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize': 500,\n",
       " '# a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either': 501,\n",
       " '# dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).': 502,\n",
       " '#': 503,\n",
       " '# Here we explicit choose to use standard time. This matches the default behavior of': 504,\n",
       " '# pytz.': 505,\n",
       " '# Here are some code to help understand this behavior:': 506,\n",
       " '# >>> import datetime': 507,\n",
       " '# >>> import pandas as pd': 508,\n",
       " '# >>> import pytz': 509,\n",
       " '# >>>': 510,\n",
       " '# >>> t = datetime.datetime(2015, 11, 1, 1, 30)': 511,\n",
       " '# >>> ts = pd.Series([t])': 512,\n",
       " \"# >>> tz = pytz.timezone('America/New_York')\": 513,\n",
       " '# >>> ts.dt.tz_localize(tz, ambiguous=True)': 514,\n",
       " '# 0   2015-11-01 01:30:00-04:00': 515,\n",
       " '# dtype: datetime64[ns, America/New_York]': 516,\n",
       " '# >>> ts.dt.tz_localize(tz, ambiguous=False)': 517,\n",
       " '# 0   2015-11-01 01:30:00-05:00': 518,\n",
       " '# >>> str(tz.localize(t))': 519,\n",
       " \"# '2015-11-01 01:30:00-05:00'\": 520,\n",
       " 'ambiguous': 521,\n",
       " '_check_series_convert_timestamps_localize': 522,\n",
       " 'from_timezone': 523,\n",
       " 'to_timezone': 524,\n",
       " 'pd': 525,\n",
       " 'from_tz': 526,\n",
       " 'to_tz': 527,\n",
       " \"# `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\": 528,\n",
       " 'apply': 529,\n",
       " 'ts': 530,\n",
       " 'NaT': 531,\n",
       " 'add': 532,\n",
       " 'self': 533,\n",
       " 'data_type': 534,\n",
       " 'metadata': 535,\n",
       " '\"Must specify DataType if passing name of struct_field to create.\"': 536,\n",
       " 'data_type_f': 537,\n",
       " '_parse_datatype_json_value': 538,\n",
       " '# Precalculated list of fields that need conversion with fromInternal/toInternal functions': 539,\n",
       " '_needConversion': 540,\n",
       " 'needConversion': 541,\n",
       " '_needSerializeAnyField': 542,\n",
       " '_cachedSqlType': 543,\n",
       " 'cls': 544,\n",
       " '\"_cached_sql_type\"': 545,\n",
       " '_cached_sql_type': 546,\n",
       " 'asDict': 547,\n",
       " 'recursive': 548,\n",
       " '\"Cannot convert a Row class into dict\"': 549,\n",
       " 'o': 550,\n",
       " 'summary': 551,\n",
       " 'hasSummary': 552,\n",
       " 'LinearRegressionTrainingSummary': 553,\n",
       " 'super': 554,\n",
       " 'LinearRegressionModel': 555,\n",
       " 'RuntimeError': 556,\n",
       " '\"No training summary available for this %s\"': 557,\n",
       " '__class__': 558,\n",
       " '__name__': 559,\n",
       " 'evaluate': 560,\n",
       " 'dataset': 561,\n",
       " 'DataFrame': 562,\n",
       " '\"dataset must be a DataFrame but got %s.\"': 563,\n",
       " 'java_lr_summary': 564,\n",
       " '_call_java': 565,\n",
       " '\"evaluate\"': 566,\n",
       " 'LinearRegressionSummary': 567,\n",
       " 'GeneralizedLinearRegressionTrainingSummary': 568,\n",
       " 'GeneralizedLinearRegressionModel': 569,\n",
       " 'java_glr_summary': 570,\n",
       " 'GeneralizedLinearRegressionSummary': 571,\n",
       " '_get_local_dirs': 572,\n",
       " 'sub': 573,\n",
       " 'environ': 574,\n",
       " '\"SPARK_LOCAL_DIRS\"': 575,\n",
       " '\"/tmp\"': 576,\n",
       " 'dirs': 577,\n",
       " 'split': 578,\n",
       " '\",\"': 579,\n",
       " '# different order in different processes and instances': 580,\n",
       " 'rnd': 581,\n",
       " 'random': 582,\n",
       " 'Random': 583,\n",
       " 'getpid': 584,\n",
       " 'id': 585,\n",
       " 'shuffle': 586,\n",
       " '\"python\"': 587,\n",
       " '_get_spill_dir': 588,\n",
       " 'localdirs': 589,\n",
       " 'mergeValues': 590,\n",
       " 'iterator': 591,\n",
       " '# speedup attribute lookup': 592,\n",
       " 'creator': 593,\n",
       " 'comb': 594,\n",
       " 'agg': 595,\n",
       " 'createCombiner': 596,\n",
       " 'mergeValue': 597,\n",
       " 'c': 598,\n",
       " 'data': 599,\n",
       " 'pdata': 600,\n",
       " 'hfun': 601,\n",
       " 'batch': 602,\n",
       " '_partition': 603,\n",
       " 'limit': 604,\n",
       " 'memory_limit': 605,\n",
       " '+=': 606,\n",
       " '>=': 607,\n",
       " 'get_used_memory': 608,\n",
       " '_spill': 609,\n",
       " '_next_limit': 610,\n",
       " '/=': 611,\n",
       " '*=': 612,\n",
       " '1.5': 613,\n",
       " 'mergeCombiners': 614,\n",
       " 'objsize': 615,\n",
       " '_object_size': 616,\n",
       " 'global': 617,\n",
       " 'MemoryBytesSpilled': 618,\n",
       " 'DiskBytesSpilled': 619,\n",
       " 'spills': 620,\n",
       " 'exists': 621,\n",
       " 'makedirs': 622,\n",
       " 'used_memory': 623,\n",
       " '# The data has not been partitioned, it will iterator the': 624,\n",
       " '# dataset once, write them into different files, has no': 625,\n",
       " '# additional memory. It only called when the memory goes': 626,\n",
       " '# above limit at the first time.': 627,\n",
       " '# open all the files for writing': 628,\n",
       " 'streams': 629,\n",
       " 'partitions': 630,\n",
       " 'h': 631,\n",
       " '# put one item in batch, make it compatible with load_stream': 632,\n",
       " '# it will increase the memory if dump them in batch': 633,\n",
       " 'serializer': 634,\n",
       " 'dump_stream': 635,\n",
       " 'tell': 636,\n",
       " 'close': 637,\n",
       " 'clear': 638,\n",
       " '\"wb\"': 639,\n",
       " '# dump items in batch': 640,\n",
       " 'iter': 641,\n",
       " 'getsize': 642,\n",
       " 'gc': 643,\n",
       " 'collect': 644,\n",
       " '# release the memory as much as possible': 645,\n",
       " '<<': 646,\n",
       " '20': 647,\n",
       " '_external_items': 648,\n",
       " '# disable partitioning and spilling when merge combiners from disk': 649,\n",
       " '_merged_items': 650,\n",
       " 'yield': 651,\n",
       " '# remove the merged partition': 652,\n",
       " 'j': 653,\n",
       " 'remove': 654,\n",
       " 'finally': 655,\n",
       " '_cleanup': 656,\n",
       " '_recursive_merged_items': 657,\n",
       " 'index': 658,\n",
       " 'subdirs': 659,\n",
       " '\"parts\"': 660,\n",
       " 'm': 661,\n",
       " 'ExternalMerger': 662,\n",
       " '*': 663,\n",
       " 'load_stream': 664,\n",
       " '_get_path': 665,\n",
       " 'local_dirs': 666,\n",
       " 'reverse': 667,\n",
       " '100': 668,\n",
       " 'chunks': 669,\n",
       " 'current_chunk': 670,\n",
       " 'while': 671,\n",
       " '# pick elements in batch': 672,\n",
       " 'chunk': 673,\n",
       " 'itertools': 674,\n",
       " 'islice': 675,\n",
       " 'break': 676,\n",
       " '# sort them inplace will save memory': 677,\n",
       " 'sort': 678,\n",
       " '# close the file explicit once we consume all the items': 679,\n",
       " '# to avoid ResourceWarning in Python3': 680,\n",
       " 'unlink': 681,\n",
       " '# data will be deleted after close': 682,\n",
       " '10000': 683,\n",
       " 'heapq': 684,\n",
       " 'merge': 685,\n",
       " '_file': 686,\n",
       " '_open_file': 687,\n",
       " 'pos': 688,\n",
       " '_ser': 689,\n",
       " 'values': 690,\n",
       " '# data once, write them into different files, has no': 691,\n",
       " '# If the number of keys is small, then the overhead of sort is small': 692,\n",
       " '# sort them before dumping into disks': 693,\n",
       " '_sorted': 694,\n",
       " 'SORT_KEY_LIMIT': 695,\n",
       " 'flattened_serializer': 696,\n",
       " 'keys': 697,\n",
       " '# self.pdata is cached in `mergeValues` and `mergeCombiners`': 698,\n",
       " '# sort by key only (stable)': 699,\n",
       " 'sorted_items': 700,\n",
       " 'operator': 701,\n",
       " 'itemgetter': 702,\n",
       " '_merge_sorted_items': 703,\n",
       " 'load_partition': 704,\n",
       " '65536': 705,\n",
       " 'disk_items': 706,\n",
       " '# all the partitions are already sorted': 707,\n",
       " '# Flatten the combined values, so it will not consume huge': 708,\n",
       " '# memory during merging sort.': 709,\n",
       " 'ser': 710,\n",
       " 'sorter': 711,\n",
       " 'ExternalSorter': 712,\n",
       " 'chain': 713,\n",
       " 'vs': 714,\n",
       " 'GroupByKey': 715,\n",
       " 'worker': 716,\n",
       " 'sock': 717,\n",
       " 'authenticated': 718,\n",
       " 'signal': 719,\n",
       " 'SIGHUP': 720,\n",
       " 'SIG_DFL': 721,\n",
       " 'SIGCHLD': 722,\n",
       " 'SIGTERM': 723,\n",
       " '# restore the handler for SIGINT,': 724,\n",
       " \"# it's useful for debugging (show the stacktrace before exit)\": 725,\n",
       " 'SIGINT': 726,\n",
       " 'default_int_handler': 727,\n",
       " '# Read the socket using fdopen instead of socket.makefile() because the latter': 728,\n",
       " '# seems to be very slow; note that we need to dup() the file descriptor because': 729,\n",
       " '# otherwise writes also cause a seek that makes us miss data on the read side.': 730,\n",
       " 'infile': 731,\n",
       " 'fdopen': 732,\n",
       " 'dup': 733,\n",
       " 'fileno': 734,\n",
       " '\"rb\"': 735,\n",
       " 'outfile': 736,\n",
       " 'client_secret': 737,\n",
       " 'UTF8Deserializer': 738,\n",
       " 'loads': 739,\n",
       " '\"PYTHON_WORKER_FACTORY_SECRET\"': 740,\n",
       " 'write_with_length': 741,\n",
       " '\"ok\"': 742,\n",
       " '\"utf-8\"': 743,\n",
       " 'flush': 744,\n",
       " '\"err\"': 745,\n",
       " 'exit_code': 746,\n",
       " 'worker_main': 747,\n",
       " 'SystemExit': 748,\n",
       " 'exc': 749,\n",
       " 'compute_real_exit_code': 750,\n",
       " 'code': 751,\n",
       " 'pass': 752,\n",
       " 'portable_hash': 753,\n",
       " 'sys': 754,\n",
       " 'version_info': 755,\n",
       " \"'PYTHONHASHSEED'\": 756,\n",
       " '\"Randomness of hash of string should be disabled via PYTHONHASHSEED\"': 757,\n",
       " '0x345678': 758,\n",
       " '^=': 759,\n",
       " '1000003': 760,\n",
       " '&=': 761,\n",
       " 'maxsize': 762,\n",
       " 'hash': 763,\n",
       " '_parse_memory': 764,\n",
       " 'units': 765,\n",
       " \"'g'\": 766,\n",
       " '1024': 767,\n",
       " \"'m'\": 768,\n",
       " \"'t'\": 769,\n",
       " \"'k'\": 770,\n",
       " '1.0': 771,\n",
       " '/': 772,\n",
       " 'lower': 773,\n",
       " '\"invalid format: \"': 774,\n",
       " 'float': 775,\n",
       " 'ignore_unicode_prefix': 776,\n",
       " 'version': 777,\n",
       " \"'3'\": 778,\n",
       " \"# the representation of unicode string in Python 3 does not have prefix 'u',\": 779,\n",
       " \"# so remove the prefix 'u' for doc tests\": 780,\n",
       " 'literal_re': 781,\n",
       " 're': 782,\n",
       " 'compile': 783,\n",
       " 'r\"(\\\\W|^)[uU]([\\'])\"': 784,\n",
       " 'UNICODE': 785,\n",
       " '__doc__': 786,\n",
       " \"r'\\\\1\\\\2'\": 787,\n",
       " 'cache': 788,\n",
       " 'is_cached': 789,\n",
       " 'persist': 790,\n",
       " 'StorageLevel': 791,\n",
       " 'MEMORY_ONLY': 792,\n",
       " 'storageLevel': 793,\n",
       " 'javaStorageLevel': 794,\n",
       " 'ctx': 795,\n",
       " '_getJavaStorageLevel': 796,\n",
       " '_jrdd': 797,\n",
       " 'unpersist': 798,\n",
       " 'blocking': 799,\n",
       " 'getCheckpointFile': 800,\n",
       " 'checkpointFile': 801,\n",
       " 'rdd': 802,\n",
       " 'isDefined': 803,\n",
       " 'preservesPartitioning': 804,\n",
       " 'func': 805,\n",
       " 'fail_on_stopiteration': 806,\n",
       " 'mapPartitionsWithIndex': 807,\n",
       " 'flatMap': 808,\n",
       " 'from_iterable': 809,\n",
       " 'mapPartitions': 810,\n",
       " 'mapPartitionsWithSplit': 811,\n",
       " 'warnings': 812,\n",
       " 'warn': 813,\n",
       " '\"mapPartitionsWithSplit is deprecated; \"': 814,\n",
       " '\"use mapPartitionsWithIndex instead\"': 815,\n",
       " 'DeprecationWarning': 816,\n",
       " 'stacklevel': 817,\n",
       " 'distinct': 818,\n",
       " 'numPartitions': 819,\n",
       " 'reduceByKey': 820,\n",
       " 'sample': 821,\n",
       " 'withReplacement': 822,\n",
       " 'fraction': 823,\n",
       " 'seed': 824,\n",
       " '0.0': 825,\n",
       " '\"Negative fraction value: %s\"': 826,\n",
       " 'RDDSampler': 827,\n",
       " 'randomSplit': 828,\n",
       " 'sum': 829,\n",
       " 'cweights': 830,\n",
       " 'w': 831,\n",
       " 'randint': 832,\n",
       " '**': 833,\n",
       " 'RDDRangeSampler': 834,\n",
       " 'lb': 835,\n",
       " 'ub': 836,\n",
       " 'takeSample': 837,\n",
       " 'num': 838,\n",
       " 'numStDev': 839,\n",
       " '10.0': 840,\n",
       " '\"Sample size cannot be negative.\"': 841,\n",
       " 'initialCount': 842,\n",
       " 'count': 843,\n",
       " 'rand': 844,\n",
       " '# shuffle current RDD and return': 845,\n",
       " 'samples': 846,\n",
       " 'maxSampleSize': 847,\n",
       " '\"Sample size cannot be greater than %d.\"': 848,\n",
       " 'RDD': 849,\n",
       " '_computeFractionForSampleSize': 850,\n",
       " \"# If the first sample didn't turn out large enough, keep trying to take samples;\": 851,\n",
       " \"# this shouldn't happen often because we use a big multiplier for their initial size.\": 852,\n",
       " '# See: scala/spark/RDD.scala': 853,\n",
       " '# TODO: add log warning for when more than one iteration was run': 854,\n",
       " 'sampleSizeLowerBound': 855,\n",
       " 'total': 856,\n",
       " '12': 857,\n",
       " '9': 858,\n",
       " 'delta': 859,\n",
       " '0.00005': 860,\n",
       " 'gamma': 861,\n",
       " 'log': 862,\n",
       " 'union': 863,\n",
       " 'other': 864,\n",
       " '_jrdd_deserializer': 865,\n",
       " '# These RDDs contain data in different serialized formats, so we': 866,\n",
       " '# must normalize them to the default serializer.': 867,\n",
       " 'self_copy': 868,\n",
       " '_reserialize': 869,\n",
       " 'other_copy': 870,\n",
       " 'partitioner': 871,\n",
       " 'getNumPartitions': 872,\n",
       " 'intersection': 873,\n",
       " 'cogroup': 874,\n",
       " 'filter': 875,\n",
       " 'k_vs': 876,\n",
       " 'all': 877,\n",
       " 'repartitionAndSortWithinPartitions': 878,\n",
       " 'partitionFunc': 879,\n",
       " 'ascending': 880,\n",
       " 'keyfunc': 881,\n",
       " '_defaultReducePartitions': 882,\n",
       " 'memory': 883,\n",
       " '_conf': 884,\n",
       " '\"spark.python.worker.memory\"': 885,\n",
       " '\"512m\"': 886,\n",
       " 'sortPartition': 887,\n",
       " '0.9': 888,\n",
       " 'k_v': 889,\n",
       " 'partitionBy': 890,\n",
       " 'sortByKey': 891,\n",
       " '_memory_limit': 892,\n",
       " 'kv': 893,\n",
       " 'coalesce': 894,\n",
       " '# first compute the boundary of each part via sampling: we want to partition': 895,\n",
       " '# the key-space into bins such that the bins have roughly the same': 896,\n",
       " '# number of (key, value) pairs falling into them': 897,\n",
       " 'rddSize': 898,\n",
       " '# empty RDD': 899,\n",
       " '20.0': 900,\n",
       " \"# constant from Spark's RangePartitioner\": 901,\n",
       " '# we have numPartitions many parts but one of the them has': 902,\n",
       " '# an implicit boundary': 903,\n",
       " 'bounds': 904,\n",
       " 'rangePartitioner': 905,\n",
       " 'bisect': 906,\n",
       " 'bisect_left': 907,\n",
       " 'sortBy': 908,\n",
       " 'keyBy': 909,\n",
       " 'cartesian': 910,\n",
       " \"# Due to batching, we can't use the Java cartesian method.\": 911,\n",
       " 'deserializer': 912,\n",
       " 'CartesianDeserializer': 913,\n",
       " 'groupBy': 914,\n",
       " 'groupByKey': 915,\n",
       " 'pipe': 916,\n",
       " 'command': 917,\n",
       " 'env': 918,\n",
       " 'checkCode': 919,\n",
       " 'Popen': 920,\n",
       " 'shlex': 921,\n",
       " 'stdin': 922,\n",
       " 'PIPE': 923,\n",
       " 'stdout': 924,\n",
       " 'pipe_objs': 925,\n",
       " 'out': 926,\n",
       " 'unicode': 927,\n",
       " 'rstrip': 928,\n",
       " \"'\\\\n'\": 929,\n",
       " 'write': 930,\n",
       " \"'utf-8'\": 931,\n",
       " 'Thread': 932,\n",
       " 'target': 933,\n",
       " 'args': 934,\n",
       " 'start': 935,\n",
       " 'check_return_code': 936,\n",
       " 'wait': 937,\n",
       " 'returncode': 938,\n",
       " '\"Pipe function `%s\\' exited \"': 939,\n",
       " '\"with error code %d\"': 940,\n",
       " \"b'\\\\n'\": 941,\n",
       " 'decode': 942,\n",
       " 'readline': 943,\n",
       " \"b''\": 944,\n",
       " 'foreach': 945,\n",
       " 'processPartition': 946,\n",
       " 'foreachPartition': 947,\n",
       " 'it': 948,\n",
       " 'r': 949,\n",
       " 'SCCallSiteSync': 950,\n",
       " 'context': 951,\n",
       " 'sock_info': 952,\n",
       " 'PythonRDD': 953,\n",
       " 'collectAndServe': 954,\n",
       " '_load_from_socket': 955,\n",
       " 'reduce': 956,\n",
       " 'initial': 957,\n",
       " 'next': 958,\n",
       " 'StopIteration': 959,\n",
       " 'vals': 960,\n",
       " '\"Can not reduce() empty RDD\"': 961,\n",
       " 'treeReduce': 962,\n",
       " 'depth': 963,\n",
       " '\"Depth cannot be smaller than 1 but got %d.\"': 964,\n",
       " 'zeroValue': 965,\n",
       " '# Use the second entry to indicate whether this is a dummy value.': 966,\n",
       " 'op': 967,\n",
       " 'reduced': 968,\n",
       " 'treeAggregate': 969,\n",
       " '\"Cannot reduce empty RDD.\"': 970,\n",
       " 'fold': 971,\n",
       " 'acc': 972,\n",
       " '# collecting result of mapPartitions here ensures that the copy of': 973,\n",
       " '# zeroValue provided to each partition is unique from the one provided': 974,\n",
       " '# to the final reduce call': 975,\n",
       " 'aggregate': 976,\n",
       " 'seqOp': 977,\n",
       " 'combOp': 978,\n",
       " 'aggregatePartition': 979,\n",
       " 'partiallyAggregated': 980,\n",
       " 'ceil': 981,\n",
       " 'pow': 982,\n",
       " \"# If creating an extra level doesn't help reduce the wall-clock time, we stop the tree\": 983,\n",
       " '# aggregation.': 984,\n",
       " 'curNumPartitions': 985,\n",
       " 'mapPartition': 986,\n",
       " 'a': 987,\n",
       " 'b': 988,\n",
       " 'stats': 989,\n",
       " 'redFunc': 990,\n",
       " 'left_counter': 991,\n",
       " 'right_counter': 992,\n",
       " 'mergeStats': 993,\n",
       " 'StatCounter': 994,\n",
       " 'histogram': 995,\n",
       " 'buckets': 996,\n",
       " '\"number of buckets must be >= 1\"': 997,\n",
       " '# filter out non-comparable elements': 998,\n",
       " 'comparable': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see some of these ar wrong which means we'll have to probably preprocess it outselves.\n",
    "vocab_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SOS': 0,\n",
       " 'Trains': 1,\n",
       " 'a': 2,\n",
       " 'k': 3,\n",
       " '-': 4,\n",
       " 'nearest': 5,\n",
       " 'neighbors': 6,\n",
       " 'classifier': 7,\n",
       " 'for': 8,\n",
       " 'face': 9,\n",
       " 'recognition': 10,\n",
       " '.': 11,\n",
       " 'Recognizes': 12,\n",
       " 'faces': 13,\n",
       " 'in': 14,\n",
       " 'given': 15,\n",
       " 'image': 16,\n",
       " 'using': 17,\n",
       " 'trained': 18,\n",
       " 'KNN': 19,\n",
       " 'Shows': 20,\n",
       " 'the': 21,\n",
       " 'results': 22,\n",
       " 'visually': 23,\n",
       " 'Convert': 24,\n",
       " 'dlib': 25,\n",
       " 'rect': 26,\n",
       " 'object': 27,\n",
       " 'to': 28,\n",
       " 'plain': 29,\n",
       " 'tuple': 30,\n",
       " '(': 31,\n",
       " 'top': 32,\n",
       " 'right': 33,\n",
       " 'bottom': 34,\n",
       " 'left': 35,\n",
       " ')': 36,\n",
       " 'order': 37,\n",
       " 'Make': 38,\n",
       " 'sure': 39,\n",
       " 'is': 40,\n",
       " 'within': 41,\n",
       " 'bounds': 42,\n",
       " 'of': 43,\n",
       " 'Given': 44,\n",
       " 'list': 45,\n",
       " 'encodings': 46,\n",
       " 'compare': 47,\n",
       " 'them': 48,\n",
       " 'known': 49,\n",
       " 'encoding': 50,\n",
       " 'and': 51,\n",
       " 'get': 52,\n",
       " 'euclidean': 53,\n",
       " 'distance': 54,\n",
       " 'each': 55,\n",
       " 'comparison': 56,\n",
       " 'The': 57,\n",
       " 'tells': 58,\n",
       " 'you': 59,\n",
       " 'how': 60,\n",
       " 'similar': 61,\n",
       " 'are': 62,\n",
       " 'Loads': 63,\n",
       " 'an': 64,\n",
       " 'file': 65,\n",
       " 'jpg': 66,\n",
       " 'png': 67,\n",
       " 'etc': 68,\n",
       " 'into': 69,\n",
       " 'numpy': 70,\n",
       " 'array': 71,\n",
       " 'Returns': 72,\n",
       " 'bounding': 73,\n",
       " 'boxes': 74,\n",
       " 'human': 75,\n",
       " '2d': 76,\n",
       " 'cnn': 77,\n",
       " 'detector': 78,\n",
       " 'If': 79,\n",
       " 'GPU': 80,\n",
       " 'this': 81,\n",
       " 'can': 82,\n",
       " 'give': 83,\n",
       " 'much': 84,\n",
       " 'faster': 85,\n",
       " 'since': 86,\n",
       " 'process': 87,\n",
       " 'batches': 88,\n",
       " 'images': 89,\n",
       " 'at': 90,\n",
       " 'once': 91,\n",
       " 'aren': 92,\n",
       " 't': 93,\n",
       " 'don': 94,\n",
       " 'need': 95,\n",
       " 'function': 96,\n",
       " 'returns': 97,\n",
       " 'dict': 98,\n",
       " 'feature': 99,\n",
       " 'locations': 100,\n",
       " 'eyes': 101,\n",
       " 'nose': 102,\n",
       " 'return': 103,\n",
       " '128': 104,\n",
       " 'dimension': 105,\n",
       " 'Parses': 106,\n",
       " 'data': 107,\n",
       " 'type': 108,\n",
       " 'string': 109,\n",
       " ':': 110,\n",
       " 'class': 111,\n",
       " 'DataType': 112,\n",
       " 'format': 113,\n",
       " 'equals': 114,\n",
       " 'simpleString': 115,\n",
       " 'except': 116,\n",
       " 'that': 117,\n",
       " 'level': 118,\n",
       " 'struct': 119,\n",
       " 'omit': 120,\n",
       " 'struct<': 121,\n",
       " '>': 122,\n",
       " 'atomic': 123,\n",
       " 'types': 124,\n",
       " 'use': 125,\n",
       " 'typeName': 126,\n",
       " '()': 127,\n",
       " 'as': 128,\n",
       " 'their': 129,\n",
       " 'e': 130,\n",
       " 'g': 131,\n",
       " 'byte': 132,\n",
       " 'instead': 133,\n",
       " 'tinyint': 134,\n",
       " 'ByteType': 135,\n",
       " 'We': 136,\n",
       " 'also': 137,\n",
       " 'int': 138,\n",
       " 'short': 139,\n",
       " 'name': 140,\n",
       " 'IntegerType': 141,\n",
       " 'Since': 142,\n",
       " 'Spark': 143,\n",
       " '2': 144,\n",
       " '3': 145,\n",
       " 'supports': 146,\n",
       " 'schema': 147,\n",
       " 'DDL': 148,\n",
       " 'formatted': 149,\n",
       " 'case': 150,\n",
       " 'insensitive': 151,\n",
       " 'strings': 152,\n",
       " 'Return': 153,\n",
       " 'Catalyst': 154,\n",
       " 'datatype': 155,\n",
       " 'from': 156,\n",
       " 'size': 157,\n",
       " 'integers': 158,\n",
       " 'Infer': 159,\n",
       " 'obj': 160,\n",
       " '/': 161,\n",
       " 'namedtuple': 162,\n",
       " 'whether': 163,\n",
       " 'there': 164,\n",
       " 'NullType': 165,\n",
       " 'dt': 166,\n",
       " 'or': 167,\n",
       " 'not': 168,\n",
       " 'Create': 169,\n",
       " 'converter': 170,\n",
       " 'drop': 171,\n",
       " 'names': 172,\n",
       " 'fields': 173,\n",
       " 'verifier': 174,\n",
       " 'checks': 175,\n",
       " 'against': 176,\n",
       " 'dataType': 177,\n",
       " 'raises': 178,\n",
       " 'TypeError': 179,\n",
       " 'if': 180,\n",
       " 'they': 181,\n",
       " 'do': 182,\n",
       " 'match': 183,\n",
       " 'pyarrow': 184,\n",
       " 'Arrow': 185,\n",
       " 'timezone': 186,\n",
       " 'aware': 187,\n",
       " 'timestamps': 188,\n",
       " 'naive': 189,\n",
       " 'specified': 190,\n",
       " 'local': 191,\n",
       " 'tz': 192,\n",
       " 'timestamp': 193,\n",
       " 'UTC': 194,\n",
       " 'normalized': 195,\n",
       " 'internal': 196,\n",
       " 'storage': 197,\n",
       " 'Construct': 198,\n",
       " 'StructType': 199,\n",
       " 'by': 200,\n",
       " 'adding': 201,\n",
       " 'new': 202,\n",
       " 'elements': 203,\n",
       " 'it': 204,\n",
       " 'define': 205,\n",
       " 'method': 206,\n",
       " 'accepts': 207,\n",
       " 'either': 208,\n",
       " 'Cache': 209,\n",
       " 'sqlType': 210,\n",
       " 'because': 211,\n",
       " 's': 212,\n",
       " 'heavy': 213,\n",
       " 'used': 214,\n",
       " 'toInternal': 215,\n",
       " 'Gets': 216,\n",
       " 'summary': 217,\n",
       " 'residuals': 218,\n",
       " 'mse': 219,\n",
       " 'r': 220,\n",
       " 'squared': 221,\n",
       " 'model': 222,\n",
       " 'on': 223,\n",
       " 'training': 224,\n",
       " 'set': 225,\n",
       " 'An': 226,\n",
       " 'exception': 227,\n",
       " 'thrown': 228,\n",
       " 'trainingSummary': 229,\n",
       " 'None': 230,\n",
       " 'Evaluates': 231,\n",
       " 'test': 232,\n",
       " 'dataset': 233,\n",
       " 'deviance': 234,\n",
       " 'pValues': 235,\n",
       " 'Get': 236,\n",
       " 'all': 237,\n",
       " 'directories': 238,\n",
       " 'Choose': 239,\n",
       " 'one': 240,\n",
       " 'directory': 241,\n",
       " 'spill': 242,\n",
       " 'number': 243,\n",
       " 'n': 244,\n",
       " 'Combine': 245,\n",
       " 'items': 246,\n",
       " 'creator': 247,\n",
       " 'combiner': 248,\n",
       " 'Merge': 249,\n",
       " 'K': 250,\n",
       " 'V': 251,\n",
       " 'pair': 252,\n",
       " 'mergeCombiner': 253,\n",
       " 'dump': 254,\n",
       " 'already': 255,\n",
       " 'partitioned': 256,\n",
       " 'disks': 257,\n",
       " 'merged': 258,\n",
       " 'iterator': 259,\n",
       " 'merge': 260,\n",
       " 'Sort': 261,\n",
       " 'external': 262,\n",
       " 'sort': 263,\n",
       " 'when': 264,\n",
       " 'memory': 265,\n",
       " 'goes': 266,\n",
       " 'above': 267,\n",
       " 'limit': 268,\n",
       " 'values': 269,\n",
       " 'disk': 270,\n",
       " 'load': 271,\n",
       " 'partition': 272,\n",
       " 'then': 273,\n",
       " 'group': 274,\n",
       " 'key': 275,\n",
       " 'Called': 276,\n",
       " 'worker': 277,\n",
       " 'after': 278,\n",
       " 'fork': 279,\n",
       " 'This': 280,\n",
       " 'consistent': 281,\n",
       " 'hash': 282,\n",
       " 'code': 283,\n",
       " 'builtin': 284,\n",
       " 'especially': 285,\n",
       " 'with': 286,\n",
       " 'Parse': 287,\n",
       " 'supported': 288,\n",
       " 'Java': 289,\n",
       " '1g': 290,\n",
       " '200m': 291,\n",
       " 'value': 292,\n",
       " 'MiB': 293,\n",
       " 'Ignore': 294,\n",
       " 'u': 295,\n",
       " 'prefix': 296,\n",
       " 'doc': 297,\n",
       " 'tests': 298,\n",
       " 'make': 299,\n",
       " 'works': 300,\n",
       " 'both': 301,\n",
       " 'python': 302,\n",
       " 'Persist': 303,\n",
       " 'RDD': 304,\n",
       " 'default': 305,\n",
       " 'C': 306,\n",
       " '{': 307,\n",
       " 'MEMORY_ONLY': 308,\n",
       " '}': 309,\n",
       " 'Set': 310,\n",
       " 'persist': 311,\n",
       " 'its': 312,\n",
       " 'across': 313,\n",
       " 'operations': 314,\n",
       " 'first': 315,\n",
       " 'time': 316,\n",
       " 'computed': 317,\n",
       " 'only': 318,\n",
       " 'be': 319,\n",
       " 'assign': 320,\n",
       " 'does': 321,\n",
       " 'have': 322,\n",
       " 'yet': 323,\n",
       " 'no': 324,\n",
       " 'defaults': 325,\n",
       " 'Mark': 326,\n",
       " 'non': 327,\n",
       " 'persistent': 328,\n",
       " 'remove': 329,\n",
       " 'blocks': 330,\n",
       " 'which': 331,\n",
       " 'was': 332,\n",
       " 'checkpointed': 333,\n",
       " 'applying': 334,\n",
       " 'element': 335,\n",
       " 'flattening': 336,\n",
       " 'Deprecated': 337,\n",
       " 'mapPartitionsWithIndex': 338,\n",
       " 'containing': 339,\n",
       " 'distinct': 340,\n",
       " 'sampled': 341,\n",
       " 'subset': 342,\n",
       " 'Randomly': 343,\n",
       " 'splits': 344,\n",
       " 'provided': 345,\n",
       " 'weights': 346,\n",
       " 'fixed': 347,\n",
       " 'sampling': 348,\n",
       " 'rate': 349,\n",
       " 'guarantees': 350,\n",
       " 'sample': 351,\n",
       " '=': 352,\n",
       " 'sampleSizeLowerBound': 353,\n",
       " '99': 354,\n",
       " '99%': 355,\n",
       " 'union': 356,\n",
       " 'another': 357,\n",
       " 'intersection': 358,\n",
       " 'output': 359,\n",
       " 'will': 360,\n",
       " 'contain': 361,\n",
       " 'any': 362,\n",
       " 'duplicate': 363,\n",
       " 'even': 364,\n",
       " 'input': 365,\n",
       " 'RDDs': 366,\n",
       " 'did': 367,\n",
       " 'Repartition': 368,\n",
       " 'according': 369,\n",
       " 'partitioner': 370,\n",
       " 'resulting': 371,\n",
       " 'records': 372,\n",
       " 'keys': 373,\n",
       " 'Sorts': 374,\n",
       " 'assumed': 375,\n",
       " 'consist': 376,\n",
       " 'pairs': 377,\n",
       " 'keyfunc': 378,\n",
       " 'Cartesian': 379,\n",
       " 'product': 380,\n",
       " 'b': 381,\n",
       " 'where': 382,\n",
       " 'self': 383,\n",
       " 'other': 384,\n",
       " 'grouped': 385,\n",
       " 'created': 386,\n",
       " 'piping': 387,\n",
       " 'forked': 388,\n",
       " 'Applies': 389,\n",
       " 'contains': 390,\n",
       " 'Reduces': 391,\n",
       " 'commutative': 392,\n",
       " 'associative': 393,\n",
       " 'binary': 394,\n",
       " 'operator': 395,\n",
       " 'Currently': 396,\n",
       " 'reduces': 397,\n",
       " 'partitions': 398,\n",
       " 'locally': 399,\n",
       " 'multi': 400,\n",
       " 'tree': 401,\n",
       " 'pattern': 402,\n",
       " 'Aggregate': 403,\n",
       " 'neutral': 404,\n",
       " 'zero': 405,\n",
       " 'combine': 406,\n",
       " 'functions': 407,\n",
       " 'Aggregates': 408,\n",
       " 'Find': 409,\n",
       " 'maximum': 410,\n",
       " 'item': 411,\n",
       " 'minimum': 412,\n",
       " 'Add': 413,\n",
       " 'up': 414,\n",
       " 'L': 415,\n",
       " 'StatCounter': 416,\n",
       " 'captures': 417,\n",
       " 'mean': 418,\n",
       " 'variance': 419,\n",
       " 'count': 420,\n",
       " 'operation': 421,\n",
       " 'Compute': 422,\n",
       " 'histogram': 423,\n",
       " 'buckets': 424,\n",
       " 'open': 425,\n",
       " 'last': 426,\n",
       " 'closed': 427,\n",
       " '[': 428,\n",
       " '1': 429,\n",
       " '10': 430,\n",
       " '20': 431,\n",
       " '50': 432,\n",
       " ']': 433,\n",
       " 'means': 434,\n",
       " '1<': 435,\n",
       " 'x<10': 436,\n",
       " '10<': 437,\n",
       " 'x<20': 438,\n",
       " '20<': 439,\n",
       " 'x<': 440,\n",
       " 'And': 441,\n",
       " 'we': 442,\n",
       " 'would': 443,\n",
       " '0': 444,\n",
       " 'unique': 445,\n",
       " 'dictionary': 446,\n",
       " 'N': 447,\n",
       " 'ordered': 448,\n",
       " 'ascending': 449,\n",
       " 'optional': 450,\n",
       " 'Take': 451,\n",
       " 'num': 452,\n",
       " 'Output': 453,\n",
       " 'Python': 454,\n",
       " 'form': 455,\n",
       " 'Hadoop': 456,\n",
       " 'system': 457,\n",
       " 'OutputFormat': 458,\n",
       " 'API': 459,\n",
       " 'mapreduce': 460,\n",
       " 'package': 461,\n",
       " 'Keys': 462,\n",
       " 'converted': 463,\n",
       " 'user': 464,\n",
       " 'converters': 465,\n",
       " 'org': 466,\n",
       " 'apache': 467,\n",
       " 'spark': 468,\n",
       " 'api': 469,\n",
       " 'JavaToWritableConverter': 470,\n",
       " 'Key': 471,\n",
       " 'inferred': 472,\n",
       " 'conf': 473,\n",
       " 'applied': 474,\n",
       " 'base': 475,\n",
       " 'associated': 476,\n",
       " 'SparkContext': 477,\n",
       " 'create': 478,\n",
       " 'MapReduce': 479,\n",
       " 'job': 480,\n",
       " 'configuration': 481,\n",
       " 'saving': 482,\n",
       " 'hadoop': 483,\n",
       " 'io': 484,\n",
       " 'Writable': 485,\n",
       " 'convert': 486,\n",
       " 'mechanism': 487,\n",
       " 'follows': 488,\n",
       " 'Save': 489,\n",
       " 'SequenceFile': 490,\n",
       " 'serialized': 491,\n",
       " 'objects': 492,\n",
       " 'serializer': 493,\n",
       " 'pyspark': 494,\n",
       " 'serializers': 495,\n",
       " 'PickleSerializer': 496,\n",
       " 'batch': 497,\n",
       " 'text': 498,\n",
       " 'representations': 499,\n",
       " 'reduce': 500,\n",
       " 'but': 501,\n",
       " 'immediately': 502,\n",
       " 'master': 503,\n",
       " 'copy': 504,\n",
       " 'Generic': 505,\n",
       " 'custom': 506,\n",
       " 'aggregation': 507,\n",
       " 'different': 508,\n",
       " 'result': 509,\n",
       " 'U': 510,\n",
       " 'than': 511,\n",
       " 'Thus': 512,\n",
       " 'merging': 513,\n",
       " 'two': 514,\n",
       " 'former': 515,\n",
       " 'latter': 516,\n",
       " 'between': 517,\n",
       " 'To': 518,\n",
       " 'avoid': 519,\n",
       " 'allocation': 520,\n",
       " 'these': 521,\n",
       " 'allowed': 522,\n",
       " 'modify': 523,\n",
       " 'argument': 524,\n",
       " 'creating': 525,\n",
       " 'func': 526,\n",
       " 'zeroValue': 527,\n",
       " 'may': 528,\n",
       " 'added': 529,\n",
       " 'arbitrary': 530,\n",
       " 'times': 531,\n",
       " 'must': 532,\n",
       " 'change': 533,\n",
       " 'addition': 534,\n",
       " 'multiplication': 535,\n",
       " 'Group': 536,\n",
       " 'single': 537,\n",
       " 'sequence': 538,\n",
       " 'Hash': 539,\n",
       " 'numPartitions': 540,\n",
       " 'Pass': 541,\n",
       " 'through': 542,\n",
       " 'flatMap': 543,\n",
       " 'without': 544,\n",
       " 'changing': 545,\n",
       " ';': 546,\n",
       " 'retains': 547,\n",
       " 'original': 548,\n",
       " 'partitioning': 549,\n",
       " 'map': 550,\n",
       " 'via': 551,\n",
       " 'stratified': 552,\n",
       " 'variable': 553,\n",
       " 'rates': 554,\n",
       " 'fractions': 555,\n",
       " 'has': 556,\n",
       " 'matching': 557,\n",
       " 'contained': 558,\n",
       " 'reduced': 559,\n",
       " 'Zips': 560,\n",
       " 'returning': 561,\n",
       " 'second': 562,\n",
       " 'Assumes': 563,\n",
       " 'same': 564,\n",
       " 'made': 565,\n",
       " 'indices': 566,\n",
       " 'generated': 567,\n",
       " 'Long': 568,\n",
       " 'ids': 569,\n",
       " 'current': 570,\n",
       " 'during': 571,\n",
       " 'tasks': 572,\n",
       " 'groupBy': 573,\n",
       " 'parallelism': 574,\n",
       " 'll': 575,\n",
       " 'defaultParallelism': 576,\n",
       " 'otherwise': 577,\n",
       " 'done': 578,\n",
       " 'efficiently': 579,\n",
       " 'searching': 580,\n",
       " 'maps': 581,\n",
       " 'JavaRDD': 582,\n",
       " 'Object': 583,\n",
       " 'unpickling': 584,\n",
       " '..': 585,\n",
       " 'note': 586,\n",
       " '::': 587,\n",
       " 'Experimental': 588,\n",
       " 'consume': 589,\n",
       " 'largest': 590,\n",
       " 'Column': 591,\n",
       " 'JVM': 592,\n",
       " 'Seq': 593,\n",
       " 'Scala': 594,\n",
       " 'List': 595,\n",
       " 'unary': 596,\n",
       " 'side': 597,\n",
       " 'substring': 598,\n",
       " 'column': 599,\n",
       " 'A': 600,\n",
       " 'boolean': 601,\n",
       " 'expression': 602,\n",
       " 'evaluated': 603,\n",
       " 'true': 604,\n",
       " 'arguments': 605,\n",
       " 'aliased': 606,\n",
       " 'expressions': 607,\n",
       " 'more': 608,\n",
       " 'such': 609,\n",
       " 'explode': 610,\n",
       " 'conditions': 611,\n",
       " 'multiple': 612,\n",
       " 'possible': 613,\n",
       " 'invoked': 614,\n",
       " 'returned': 615,\n",
       " 'unmatched': 616,\n",
       " 'Define': 617,\n",
       " 'windowing': 618,\n",
       " 'transformation': 619,\n",
       " 'vector': 620,\n",
       " 'Vector': 621,\n",
       " 'Computes': 622,\n",
       " 'stores': 623,\n",
       " 'later': 624,\n",
       " 'scaling': 625,\n",
       " 'ChiSquared': 626,\n",
       " 'selector': 627,\n",
       " '[[': 628,\n",
       " 'PCAModel': 629,\n",
       " ']]': 630,\n",
       " 'principal': 631,\n",
       " 'components': 632,\n",
       " 'vectors': 633,\n",
       " 'param': 634,\n",
       " 'source': 635,\n",
       " 'Transforms': 636,\n",
       " 'document': 637,\n",
       " 'terms': 638,\n",
       " 'term': 639,\n",
       " 'frequency': 640,\n",
       " 'transform': 641,\n",
       " 'inverse': 642,\n",
       " 'synonyms': 643,\n",
       " 'word': 644,\n",
       " 'Load': 645,\n",
       " 'path': 646,\n",
       " 'Hadamard': 647,\n",
       " 'Predict': 648,\n",
       " 'point': 649,\n",
       " 'points': 650,\n",
       " 'Train': 651,\n",
       " 'decision': 652,\n",
       " 'classification': 653,\n",
       " 'regression': 654,\n",
       " 'random': 655,\n",
       " 'forest': 656,\n",
       " 'multiclass': 657,\n",
       " 'gradient': 658,\n",
       " 'boosted': 659,\n",
       " 'trees': 660,\n",
       " 'property': 661,\n",
       " 'environment': 662,\n",
       " 'passed': 663,\n",
       " 'executors': 664,\n",
       " 'parameters': 665,\n",
       " 'configured': 666,\n",
       " 'some': 667,\n",
       " 'Does': 668,\n",
       " 'key?': 669,\n",
       " 'printable': 670,\n",
       " 'version': 671,\n",
       " 'per': 672,\n",
       " 'line': 673,\n",
       " 'databases': 674,\n",
       " 'available': 675,\n",
       " 'sessions': 676,\n",
       " 'tables': 677,\n",
       " 'views': 678,\n",
       " 'database': 679,\n",
       " 'registered': 680,\n",
       " 'columns': 681,\n",
       " 'table': 682,\n",
       " 'view': 683,\n",
       " 'Creates': 684,\n",
       " 'based': 685,\n",
       " 'socket': 686,\n",
       " 'blocking': 687,\n",
       " 'thus': 688,\n",
       " 'connection': 689,\n",
       " 'been': 690,\n",
       " 'Internal': 691,\n",
       " 'global': 692,\n",
       " 'BarrierTaskContext': 693,\n",
       " 'here': 694,\n",
       " 'needed': 695,\n",
       " 'reuse': 696,\n",
       " 'scenario': 697,\n",
       " 'see': 698,\n",
       " 'SPARK': 699,\n",
       " '25921': 700,\n",
       " 'details': 701,\n",
       " 'Initialize': 702,\n",
       " 'methods': 703,\n",
       " 'called': 704,\n",
       " 'initialized': 705,\n",
       " 'decorator': 706,\n",
       " 'annotates': 707,\n",
       " 'append': 708,\n",
       " 'globals': 709,\n",
       " 'closure': 710,\n",
       " 'provide': 711,\n",
       " 'forces': 712,\n",
       " 'keyword': 713,\n",
       " 'wrapped': 714,\n",
       " 'saves': 715,\n",
       " 'actual': 716,\n",
       " '_input_kwargs': 717,\n",
       " 'Generates': 718,\n",
       " 'header': 719,\n",
       " 'part': 720,\n",
       " 'shared': 721,\n",
       " 'variables': 722,\n",
       " 'Runs': 723,\n",
       " 'bisecting': 724,\n",
       " 'algorithm': 725,\n",
       " 'clustering': 726,\n",
       " 'Gaussian': 727,\n",
       " 'Mixture': 728,\n",
       " 'rdd': 729,\n",
       " 'i': 730,\n",
       " 'j': 731,\n",
       " '\\\\': 732,\n",
       " 'sub': 733,\n",
       " 'ij': 734,\n",
       " 'tuples': 735,\n",
       " 'representing': 736,\n",
       " 'affinity': 737,\n",
       " 'matrix': 738,\n",
       " 'PIC': 739,\n",
       " 'paper': 740,\n",
       " 'similarity': 741,\n",
       " 'nonnegative': 742,\n",
       " 'symmetric': 743,\n",
       " 'hence': 744,\n",
       " 'ji': 745,\n",
       " 'For': 746,\n",
       " 'nonzero': 747,\n",
       " 'should': 748,\n",
       " 'Tuples': 749,\n",
       " 'ignored': 750,\n",
       " 'Number': 751,\n",
       " 'clusters': 752,\n",
       " 'maxIterations': 753,\n",
       " 'Maximum': 754,\n",
       " 'iterations': 755,\n",
       " '100': 756,\n",
       " 'initMode': 757,\n",
       " 'Initialization': 758,\n",
       " 'mode': 759,\n",
       " 'vertex': 760,\n",
       " 'properties': 761,\n",
       " 'degree': 762,\n",
       " 'sum': 763,\n",
       " 'similarities': 764,\n",
       " 'Update': 765,\n",
       " 'centroids': 766,\n",
       " 'particular': 767,\n",
       " 'half': 768,\n",
       " 'weightage': 769,\n",
       " 'initial': 770,\n",
       " 'centers': 771,\n",
       " 'Should': 772,\n",
       " 'before': 773,\n",
       " 'calling': 774,\n",
       " 'trainOn': 775,\n",
       " 'centres': 776,\n",
       " 'samples': 777,\n",
       " 'gaussian': 778,\n",
       " 'population': 779,\n",
       " 'constant': 780,\n",
       " 'incoming': 781,\n",
       " 'dstream': 782,\n",
       " 'predictions': 783,\n",
       " 'transformed': 784,\n",
       " 'keyed': 785,\n",
       " 'topics': 786,\n",
       " 'described': 787,\n",
       " 'weighted': 788,\n",
       " 'LDAModel': 789,\n",
       " 'LDA': 790,\n",
       " 'Call': 791,\n",
       " 'Function': 792,\n",
       " 'PythonMLLibAPI': 793,\n",
       " 'makes': 794,\n",
       " 'inherit': 795,\n",
       " 'documentation': 796,\n",
       " 'parents': 797,\n",
       " 'java_model': 798,\n",
       " 'DStream': 799,\n",
       " 'counting': 800,\n",
       " 'satisfy': 801,\n",
       " 'predicate': 802,\n",
       " 'reducing': 803,\n",
       " 'reduceByKey': 804,\n",
       " 'combineByKey': 805,\n",
       " 'Apply': 806,\n",
       " 'Print': 807,\n",
       " 'Enable': 808,\n",
       " 'periodic': 809,\n",
       " 'checkpointing': 810,\n",
       " 'groupByKey': 811,\n",
       " 'counts': 812,\n",
       " 'representation': 813,\n",
       " 'unifying': 814,\n",
       " 'cogroup': 815,\n",
       " 'datetime': 816,\n",
       " 'unix_timestamp': 817,\n",
       " 'Time': 818,\n",
       " 'begin': 819,\n",
       " 'end': 820,\n",
       " 'included': 821,\n",
       " 'seen': 822,\n",
       " 'sliding': 823,\n",
       " 'window': 824,\n",
       " 'over': 825,\n",
       " 'windowDuration': 826,\n",
       " 'slideDuration': 827,\n",
       " 'defined': 828,\n",
       " 'Similar': 829,\n",
       " 'applies': 830,\n",
       " 'incremental': 831,\n",
       " 'state': 832,\n",
       " 'updated': 833,\n",
       " 'previous': 834,\n",
       " 'setParams': 835,\n",
       " 'minSupport': 836,\n",
       " 'minConfidence': 837,\n",
       " '8': 838,\n",
       " 'itemsCol': 839,\n",
       " 'predictionCol': 840,\n",
       " 'prediction': 841,\n",
       " 'maxPatternLength': 842,\n",
       " 'maxLocalProjDBSize': 843,\n",
       " '32000000': 844,\n",
       " 'sequenceCol': 845,\n",
       " 'CallSite': 846,\n",
       " 'call': 847,\n",
       " 'stack': 848,\n",
       " 'MLlib': 849,\n",
       " 'LabeledPoint': 850,\n",
       " 'f': 851,\n",
       " 'measure': 852,\n",
       " 'precision': 853,\n",
       " 'label': 854,\n",
       " 'category': 855,\n",
       " 'recall': 856,\n",
       " 'f1Measure': 857,\n",
       " 'When': 858,\n",
       " 'converting': 859,\n",
       " 'SQL': 860,\n",
       " 'Pandas': 861,\n",
       " 'DataFrame': 862,\n",
       " 'wrong': 863,\n",
       " 'gets': 864,\n",
       " 'corrected': 865,\n",
       " 'uncorrectly': 866,\n",
       " 'content': 867,\n",
       " 'Row': 868,\n",
       " 'Converts': 869,\n",
       " 'sql': 870,\n",
       " 'Prints': 871,\n",
       " 'logical': 872,\n",
       " 'physical': 873,\n",
       " 'plans': 874,\n",
       " 'console': 875,\n",
       " 'debugging': 876,\n",
       " 'purpose': 877,\n",
       " 'rows': 878,\n",
       " 'while': 879,\n",
       " 'preserving': 880,\n",
       " 'duplicates': 881,\n",
       " 'dataframe': 882,\n",
       " 'html': 883,\n",
       " 'enabled': 884,\n",
       " 'eager': 885,\n",
       " 'evaluation': 886,\n",
       " 'repl': 887,\n",
       " 'eagerEval': 888,\n",
       " 'REPL': 889,\n",
       " 'support': 890,\n",
       " 'HTML': 891,\n",
       " 'Dataset': 892,\n",
       " 'Checkpointing': 893,\n",
       " 'truncate': 894,\n",
       " 'plan': 895,\n",
       " 'useful': 896,\n",
       " 'iterative': 897,\n",
       " 'algorithms': 898,\n",
       " 'grow': 899,\n",
       " 'exponentially': 900,\n",
       " 'It': 901,\n",
       " 'saved': 902,\n",
       " 'files': 903,\n",
       " 'inside': 904,\n",
       " 'checkpoint': 905,\n",
       " 'setCheckpointDir': 906,\n",
       " 'Local': 907,\n",
       " 'checkpoints': 908,\n",
       " 'stored': 909,\n",
       " 'caching': 910,\n",
       " 'subsystem': 911,\n",
       " 'therefore': 912,\n",
       " 'reliable': 913,\n",
       " 'Defines': 914,\n",
       " 'event': 915,\n",
       " 'watermark': 916,\n",
       " 'tracks': 917,\n",
       " 'assume': 918,\n",
       " 'late': 919,\n",
       " 'going': 920,\n",
       " 'arrive': 921,\n",
       " 'Specifies': 922,\n",
       " 'hint': 923,\n",
       " 'Limits': 924,\n",
       " 'Sets': 925,\n",
       " 'contents': 926,\n",
       " 'MEMORY_AND_DISK': 927,\n",
       " 'Marks': 928,\n",
       " 'exactly': 929,\n",
       " 'replacement': 930,\n",
       " 'fraction': 931,\n",
       " 'stratum': 932,\n",
       " 'Selects': 933,\n",
       " 'regex': 934,\n",
       " 'alias': 935,\n",
       " 'cartesian': 936,\n",
       " 'Joins': 937,\n",
       " 'join': 938,\n",
       " 'sorted': 939,\n",
       " 'Columns': 940,\n",
       " 'describes': 941,\n",
       " 'basic': 942,\n",
       " 'statistics': 943,\n",
       " 'numeric': 944,\n",
       " 'Available': 945,\n",
       " 'stddev': 946,\n",
       " 'min': 947,\n",
       " 'max': 948,\n",
       " 'approximate': 949,\n",
       " 'percentiles': 950,\n",
       " 'percentage': 951,\n",
       " 'eg': 952,\n",
       " '75%': 953,\n",
       " 'Projects': 954,\n",
       " 'Filters': 955,\n",
       " 'condition': 956,\n",
       " 'Groups': 957,\n",
       " 'so': 958,\n",
       " 'run': 959,\n",
       " 'See': 960,\n",
       " 'GroupedData': 961,\n",
       " 'aggregate': 962,\n",
       " 'frame': 963,\n",
       " 'removed': 964,\n",
       " 'optionally': 965,\n",
       " 'considering': 966,\n",
       " 'certain': 967,\n",
       " 'omitting': 968,\n",
       " 'null': 969,\n",
       " 'dropna': 970,\n",
       " 'DataFrameNaFunctions': 971,\n",
       " 'aliases': 972,\n",
       " 'Replace': 973,\n",
       " 'na': 974,\n",
       " 'fill': 975,\n",
       " 'fillna': 976,\n",
       " 'replacing': 977,\n",
       " 'replace': 978,\n",
       " 'Values': 979,\n",
       " 'to_replace': 980,\n",
       " 'numerics': 981,\n",
       " 'booleans': 982,\n",
       " 'Value': 983,\n",
       " 'cast': 984,\n",
       " 'existing': 985,\n",
       " 'replacements': 986,\n",
       " 'replaced': 987,\n",
       " 'floating': 988,\n",
       " 'In': 989,\n",
       " 'conflicts': 990,\n",
       " 'example': 991,\n",
       " '42': 992,\n",
       " 'Calculates': 993,\n",
       " 'quantiles': 994,\n",
       " 'numerical': 995,\n",
       " 'correlation': 996,\n",
       " 'double': 997,\n",
       " 'Pearson': 998,\n",
       " 'Correlation': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Create AST\n",
    "ASTs to binary trees by the following two steps which have been adopted in [28]:  \n",
    "- a) Split nodes with more than 2 children, generate a new right child together with the old left child as its children, and then put all children except the leftmost as the children of this new node.   \n",
    "Repeat this operation in a top-down way until only nodes with 0, 1, 2 children left;\n",
    "- b) Combine nodes with 1 child with its child.  \n",
    "\n",
    "(from the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children: [<ast.FunctionDef object at 0x000002123F727FA0>]\\n\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "n = ast.parse(raw_data[0]['func_code_string'])\n",
    "print(\"children: \" + str([x for x in ast.iter_child_nodes(n)]) + \"\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c. Create Sequential Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Pack them into a data structure\n",
    "We should form Train/Test Splits, I think the `raw_data` variable already holds this. Should be something of:  \n",
    "```\n",
    "inputs[0] = {  \n",
    "'seq' : list of sequential representation of code (converted to ID from sectiion 2a, lists from 2c)\n",
    "'ast' : list of ast representation of code (converted to ID from section 2a, AST from 2b.)\n",
    "}\n",
    "make sure we padding them, and have <SOS> <EOS>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition\n",
    "Probably hardest bit, I got as many variables from the paper as I could.  \n",
    "i think a lot of it will be similar to what we did in class projects (Lab 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from paper\n",
    "batch_size = 64\n",
    "hidden_size = 512\n",
    "embedding_size = 512 #LSTM, Layers\n",
    "lr = 0.001\n",
    "skip_num = 0\n",
    "code_vocab_size = len(vocab_code)\n",
    "#code_max_len = max_code\n",
    "comment_vocab_size = len(vocab_comm)\n",
    "#comment_max_len = max_comm # This and max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Actor Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Critic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. Training Loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

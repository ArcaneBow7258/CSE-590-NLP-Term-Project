{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from paper\n",
    "batch_size = 64\n",
    "hidden_size = 512\n",
    "embedding_size = 512 #LSTM, Layers\n",
    "lr = 0.001\n",
    "skip_num = 0\n",
    "code_vocab_size = 512\n",
    "#code_max_len = max_code\n",
    "comment_vocab_size = 512\n",
    "#comment_max_len = max_comm # This and max_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoder\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, code_vocab, comm_vocab):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m#super(ActorNetwork, self).__init__()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, code_vocab, comm_vocab):\n",
    "        #super(ActorNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # these are probalby btached, investigate later\n",
    "        # Create embeeddings\n",
    "        self.enc_embedding_seq = nn.Embedding(code_vocab, hidden_size)\n",
    "        self.enc_embedding_struct = nn.Embedding(comm_vocab, hidden_size)\n",
    "\n",
    "        #encode with LSTM\n",
    "        self.enc_LSTM_seq = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.enc_LSTM_struct = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # END ENCODE\n",
    "\n",
    "    def forward(self, seq, struct):\n",
    "        '''\n",
    "        Takes Sequential and Structural representation of code and creates two hidden layers and outputs out of them.\n",
    "        '''\n",
    "        e_seq = self.enc_embedding_seq(seq)\n",
    "        e_struct = self.enc_embedding_struct(struct)\n",
    "        out_seq, hide_seq = self.enc_LSTM_seq(e_seq)\n",
    "        out_struct, hide_struct = self.enc_LSTM_struct(e_struct)\n",
    "\n",
    "        return out_seq, hide_seq, out_struct, hide_struct\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, code_vocab, comm_vocab):\n",
    "        # START DCODE\n",
    "        self.dec_embedding = nn.Embedding(comm_vocab, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dec_LSTM = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.pred = nn.Linear(hidden_size,  comm_vocab, 1) # Predict \n",
    "    def forward(self, input, hidden):\n",
    "        out = self.dec_embedding(input)\n",
    "        out = self.relu(out)\n",
    "        out, hidden = self.dec_LSTM(out, hidden)\n",
    "        out = self.pred(out)\n",
    "\n",
    "        return out, hidden \n",
    "\n",
    "def Attention(query, key, mask=None, dropout=None): # Key and Value are the? same\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \n",
    "    attention = F.softmax(scores, dim = -1)\n",
    "    # Context vector\n",
    "    return torch.bmm(attention, key), attention\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        # START DCODE\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, query, h_str, h_seq):\n",
    "        # Attention of each hidden state we look at and get context vetcor\n",
    "        d_str, a_str = Attention(query, h_str)\n",
    "        d_seq, a_seq = Attention(query, h_seq)\n",
    "        \n",
    "        # Get respective context vectors and combine together\n",
    "\n",
    "        d_t = self.linear1(torch.cat([d_str, d_seq]))\n",
    "\n",
    "        # Finally create context vector for next wordprediction\n",
    "        return self.tanh(self.linear2(torch.cat([query, d_t])))\n",
    "# scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "# #print(scores.shape)\n",
    "# scores = scores.squeeze(2).unsqueeze(1)\n",
    "# weights = F.softmax(scores, dim=-1)\n",
    "# #print(weights.shape, keys.shape)\n",
    "# context = torch.bmm(weights, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(0, hidden_size, code_vocab_size, comment_vocab_size)\n",
    "decoder = Decoder(0, hidden_size, code_vocab_size, comment_vocab_size)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

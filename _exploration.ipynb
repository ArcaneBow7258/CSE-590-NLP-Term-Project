{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Summarization with Deep Reinforcement Learning\n",
    "Paper[1] is linked: [Improving Code Summarization with Deep Reinforcement Learning](https://arxiv.org/abs/1811.07234)  \n",
    "We're attempting to add concepts from Paper[2] [Unsupervised Translation of Programming Languages](https://arxiv.org/pdf/2006.03511)  \n",
    "How? Not sure...  \n",
    "I'm thinking of using their masking technique to finetune the product of Paper[1]. Essentially, with a given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cache_dir = 'F:\\\\.cache\\\\huggingface\\\\'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "import code_search_net\n",
    "import datasets\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import ast\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the below line instead of you don't want to load locally.\n",
    "# BUt I havne't figured out to only choose certain datasets, I think its the uploader's fault\n",
    "# Total is like 10+GB's\n",
    "# d = datasets.load_dataset('code-search-net/code_search_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# What language you used for the data, otherwise default to all\n",
    "dl_manager = datasets.DownloadManager('python')\n",
    "csn = code_search_net.CodeSearchNet(config_name='python')\n",
    "# You can remove the loc variable. My c:drive has no space so I did this instead.\n",
    "generators = csn._split_generators(dl_manager, loc = 'F:\\\\.cache\\\\huggingface\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for i, data in csn._generate_examples(generators[0].gen_kwargs['filepaths'][:1]):\n",
    "    raw_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of interest:\n",
    "# func_code_tokens\n",
    "# func_documentation_tokens\n",
    "# possibly func_path_in_repository?\n",
    "print(len(raw_data))\n",
    "raw_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Inputs\n",
    "\"To convert code into sequential text, we tokenize the code by `{. , ” ’ : ; ) ( ! (space)}`, which  has been used in [8].  \n",
    "We tokenize the comment by {(space)}\" - Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Create AST\n",
    "ASTs to binary trees by the following two steps which have been adopted in [28]:  \n",
    "- a) Split nodes with more than 2 children, generate a new right child together with the old left child as its children, and then put all children except the leftmost as the children of this new node.   \n",
    "Repeat this operation in a top-down way until only nodes with 0, 1, 2 children left;\n",
    "- b) Combine nodes with 1 child with its child.  \n",
    "\n",
    "(from the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29969 31 30000\n"
     ]
    }
   ],
   "source": [
    "def parse_tree(lines):\n",
    "    #replace the DCNL & DCSP symbols\n",
    "    #need to deal with different indentations& spacings. The authors used DCSP for both '\\t' & whitespace\n",
    "\n",
    "    lines=lines.replace(' DCNL ',' \\n')\n",
    "    lines=lines.replace(' DCSP ',' \\t')\n",
    "    lines=lines.replace('DCSP ',' \\t')\n",
    "\n",
    "    #create temporary python file for every line (function) in input file\n",
    "    tree = ast.parse(lines)\n",
    "    temp_ast=ast.dump(tree)\n",
    "    \n",
    "    return temp_ast\n",
    "def clean_data(line):\n",
    "\n",
    "    clean = re.sub(r\"\"\"\n",
    "            [,.;@#?!&$()='\\\\_`:>\"%/{}*]+  # Accept one or more copies of punctuation\n",
    "            \\ *           # plus zero or more copies of a space,\n",
    "            \"\"\",\n",
    "            \" \",          # and replace it with a single space\n",
    "            line, flags=re.VERBOSE)\n",
    "    #Manually handle cases not accepted by sub\n",
    "    clean = clean.replace(\"[\", \"\")\n",
    "    clean = clean.replace(\"]\", \"\")\n",
    "    clean = clean.replace(\"-\", \"\")\n",
    "    # tokenize on white space\n",
    "    line = clean.split()\n",
    "    # convert to lower case\n",
    "    line = [word.lower() for word in line]\n",
    "    # store as string\n",
    "    return line\n",
    "    # remove empty strings\n",
    "\n",
    "errors = []\n",
    "for i, f in enumerate(raw_data):\n",
    "    try:\n",
    "        f['func_ast_string'] = parse_tree(f['func_code_string'])\n",
    "        f['func_ast_tokens'] = clean_data(f['func_ast_string'])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        errors.append(i)\n",
    "\n",
    "good_data = [x for i,x in enumerate(raw_data) if i not in errors]\n",
    "print(len(good_data), len(errors), len(raw_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['def', 'train', 'train_dir', 'model_save_path', 'none', 'n_neighbor', 'none', 'knn_algo', 'ball_tre', 'verbos', 'fals', 'x', 'y', 'for', 'class_dir', 'in', 'os', 'listdir', 'train_dir', 'if', 'not', 'os', 'path', 'isdir', 'os', 'path', 'join', 'train_dir', 'class_dir', 'continu', 'for', 'img_path', 'in', 'image_files_in_fold', 'os', 'path', 'join', 'train_dir', 'class_dir', 'imag', 'face_recognit', 'load_image_fil', 'img_path', 'face_bounding_box', 'face_recognit', 'face_loc', 'imag', 'if', 'len', 'face_bounding_box', '1', 'if', 'verbos', 'print', 'imag', 'not', 'suitabl', 'for', 'train', 'format', 'img_path', 'didn', 't', 'find', 'a', 'face', 'if', 'len', 'face_bounding_box', '<', '1', 'els', 'found', 'more', 'than', 'one', 'face', 'els', 'x', 'append', 'face_recognit', 'face_encod', 'imag', 'known_face_loc', 'face_bounding_box', '0', 'y', 'append', 'class_dir', 'if', 'n_neighbor', 'is', 'none', 'n_neighbor', 'int', 'round', 'math', 'sqrt', 'len', 'x', 'if', 'verbos', 'print', 'chose', 'n_neighbor', 'automat', 'n_neighbor', 'knn_clf', 'neighbor', 'kneighborsclassifi', 'n_neighbor', 'n_neighbor', 'algorithm', 'knn_algo', 'weight', 'distanc', 'knn_clf', 'fit', 'x', 'y', 'if', 'model_save_path', 'is', 'not', 'none', 'with', 'open', 'model_save_path', 'wb', 'as', 'f', 'pickl', 'dump', 'knn_clf', 'f', 'return', 'knn_clf'], 'def train(train_dir, model_save_path=none, n_neighbors=none, knn_algo=\\'ball_tree\\', verbose=false):\\n    \\n    x = []\\n    y = []\\n\\n    \\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        \\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                \\n                if verbose:\\n                    print(\"image {} not suitable for training: {}\".format(img_path, \"didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"found more than one face\"))\\n            else:\\n                \\n                x.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    \\n    if n_neighbors is none:\\n        n_neighbors = int(round(math.sqrt(len(x))))\\n        if verbose:\\n            print(\"chose n_neighbors automatically:\", n_neighbors)\\n\\n    \\n    knn_clf = neighbors.kneighborsclassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(x, y)\\n\\n    \\n    if model_save_path is not none:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf')\n",
      "['train', 'a', 'k-nearest', 'neighbor', 'classifi', 'for', 'face', 'recognit', 'param', 'train_dir', 'directori', 'that', 'contain', 'a', 'sub-directori', 'for', 'each', 'known', 'person', 'with', 'it', 'name', 'view', 'in', 'sourc', 'code', 'to', 'see', 'train_dir', 'exampl', 'tree', 'structur', 'structur', '<train_dir>/', '<person1>/', '<somename1>', 'jpeg', '<somename2>', 'jpeg', '<person2>/', '<somename1>', 'jpeg', '<somename2>', 'jpeg', 'param', 'model_save_path', 'option', 'path', 'to', 'save', 'model', 'on', 'disk', 'param', 'n_neighbor', 'option', 'number', 'of', 'neighbor', 'to', 'weigh', 'in', 'classif', 'chosen', 'automat', 'if', 'not', 'specifi', 'param', 'knn_algo', 'option', 'underli', 'data', 'structur', 'to', 'support', 'knn', 'default', 'is', 'ball_tre', 'param', 'verbos', 'verbos', 'of', 'train', 'return', 'return', 'knn', 'classifi', 'that', 'wa', 'train', 'on', 'the', 'given', 'data']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def token_code(func_code_string):\n",
    "    stripped = re.sub('\"\"\"[\\s\\S]*\"\"\"', '', func_code_string.lower()) # apaprently code has the entire docstring in there too\n",
    "    no_comment = re.sub('#.*', '', stripped) # We should base it off just the comments\n",
    "    tokens = re.split('[ \\[\\]{}\\.,:;\\)\\(!\\)=(\\\\n)(\\\\t)\"\\']', no_comment) # split appropaite, then next line remove ''\n",
    "    return [stemmer.stem(x) for x in filter(None, tokens)], no_comment # https://stackoverflow.com/questions/30933216/split-by-regex-without-resulting-empty-strings-in-python\n",
    "print(token_code(raw_data[0]['func_code_string']))\n",
    "def token_comm(func_doc_string):\n",
    "    stripped = re.sub('[\\[\\](\\\\n)|,:\\.─├│└(\\\\t)\"\\']', ' ', func_doc_string.lower())\n",
    "    tokens = re.split(\" \", stripped)\n",
    "    return [stemmer.stem(x) for x in filter(None, tokens)] # https://stackoverflow.com/questions/30933216/split-by-regex-without-resulting-empty-strings-in-python\n",
    "print(token_comm(raw_data[0]['func_documentation_string']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Vocab Creation\n",
    "- Reparse func_code_string into tokens\n",
    "- Might respase func_documaetnation_string\n",
    "- lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab / Max Sentence Length\n",
      "159032 6184  |  58199 4959\n"
     ]
    }
   ],
   "source": [
    "vocab_code = {'<SOS>' : 0, '<EOS>': 1} # Is this necessary?\n",
    "max_code = 0\n",
    "i = 1\n",
    "vocab_comm = {'<SOS>' : 0, '<EOS>': 1}\n",
    "max_comm = 0\n",
    "j = 1\n",
    "vocab_struct = {'<SOS>' : 0, '<EOS>': 1}\n",
    "max_struc = 0\n",
    "k = 1\n",
    "for f in good_data:\n",
    "    # I think we will need to preprocess out own toeksn btu for right now prrof of concenpt:\n",
    "    f['func_code_tokens'], f['func_code_string'] = token_code(f['func_code_string']) # replace with out tokens\n",
    "    f['func_code_tokens'] = ['<SOS>'] + f['func_code_tokens'] + ['<EOS>']\n",
    "    \n",
    "    for code_token in f['func_code_tokens']:\n",
    "        if code_token not in vocab_code.keys():\n",
    "            vocab_code[code_token] = i\n",
    "            i += 1\n",
    "    max_code = max(max_code, len(f['func_code_tokens']))\n",
    "    # With code\n",
    "    f['func_ast_tokens'] =  ['<SOS>'] + f['func_ast_tokens']  + ['<EOS>']\n",
    "    for code_token in f['func_ast_tokens']:\n",
    "        if code_token not in vocab_code.keys():\n",
    "            vocab_code[code_token] = k\n",
    "            k += 1\n",
    "    max_struc = max(max_struc, len(f['func_ast_tokens']))\n",
    "    # # Standalone\n",
    "    # f['func_ast_tokens'] =  ['<SOS>'] + f['func_ast_tokens']  + ['<EOS>']\n",
    "    # for code_token in f['func_ast_tokens']:\n",
    "    #     if code_token not in vocab_struct.keys():\n",
    "    #         vocab_struct[code_token] = k\n",
    "    #         k += 1\n",
    "    # max_struc = max(max_struc, len(f['func_ast_tokens']))\n",
    "    f['func_documentation_tokens'] =  ['<SOS>'] + token_comm(f['func_documentation_string']) + ['<EOS>']\n",
    "    for comm_token in f['func_documentation_tokens']:\n",
    "        if comm_token not in vocab_comm.keys():\n",
    "            vocab_comm[comm_token] = j\n",
    "            j += 1\n",
    "    max_comm = max(max_comm, len(f['func_documentation_tokens']))\n",
    "print('Vocab / Max Sentence Length')\n",
    "print(len(vocab_code), max_code, ' | ', len(vocab_comm), max_comm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_comm = {v: k for k, v in vocab_comm.items()} # https://stackoverflow.com/questions/483666/reverse-invert-a-dictionary-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2x Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2390, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can probalby do this faster with torch?\n",
    "MAX_LENGTH = 128\n",
    "input_seq = []\n",
    "input_str = []\n",
    "target = []\n",
    "for x in good_data:\n",
    "    temp_seq = []\n",
    "    temp_str = []\n",
    "    temp_comm = []\n",
    "    for token in x['func_code_tokens']:\n",
    "        temp_seq.append(vocab_code[token])\n",
    "    for token in x['func_ast_tokens']:\n",
    "        temp_str.append(vocab_code[token])\n",
    "        #  temp_str.append(vocab_struct[token])\n",
    "    for token in x['func_documentation_tokens']:\n",
    "        temp_comm.append(vocab_comm[token])\n",
    "    if len(temp_seq) > MAX_LENGTH or len(temp_str) > MAX_LENGTH or len(temp_comm) > MAX_LENGTH: # If too big we'll discard\n",
    "        continue\n",
    "    else: # Otherwise we'll padd them util same length\n",
    "        for x in range(len(temp_seq), MAX_LENGTH):\n",
    "            temp_seq.append(vocab_code['<EOS>'])\n",
    "        for x in range(len(temp_str), MAX_LENGTH):\n",
    "            temp_str.append(vocab_code['<EOS>'])\n",
    "        # for x in range(len(temp_str), 512):\n",
    "        #     temp_str.append(vocab_struct['<EOS>'])\n",
    "\n",
    "        for x in range(len(temp_comm), MAX_LENGTH):\n",
    "            temp_comm.append(vocab_comm['<EOS>'])\n",
    "        input_seq.append(temp_seq)\n",
    "        input_str.append(temp_str)\n",
    "        target.append(temp_comm)\n",
    "        \n",
    "input_seq = torch.tensor(input_seq, dtype=torch.int).to(device)\n",
    "input_str = torch.tensor(input_str, dtype=torch.int).to(device)\n",
    "target = torch.tensor(target, dtype=torch.int).to(device)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data import random_split\n",
    "from torch import Generator\n",
    "def create_dataloaders():\n",
    "    all_data = torch.stack([input_seq, input_str, target], dim = 1)\n",
    "    gen = Generator().manual_seed(818)\n",
    "    batch_size = 64\n",
    "    train_set, test_set = random_split(all_data, [0.7, 0.3], generator=gen)\n",
    "    train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set), batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_set, sampler=RandomSampler(test_set), batch_size=batch_size)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders()\n",
    "for x in train_dataloader:\n",
    "    print(x[:,0,:].shape) # input_seq\n",
    "    print(x[:,1,:].shape) # input_str\n",
    "    print(x[:,2,:].shape) # target\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition\n",
    "Probably hardest bit, I got as many variables from the paper as I could.  \n",
    "i think a lot of it will be similar to what we did in class projects (Lab 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a Fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, code_vocab, struct_vocab):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # these are probalby btached, investigate later\n",
    "        # Create embeeddings\n",
    "        self.enc_embedding = nn.Embedding(code_vocab, hidden_size)\n",
    "        #self.enc_embedding_struct = nn.Embedding(struct_vocab, hidden_size)\n",
    "\n",
    "        #encode with GRU\n",
    "        self.enc_GRU_seq = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.enc_GRU_struct = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        # END ENCODE\n",
    "\n",
    "    def forward(self, seq, struct):\n",
    "        '''\n",
    "        Takes Sequential and Structural representation of code and creates two hidden layers and outputs out of them.\n",
    "        '''\n",
    "        # e_seq = self.enc_embedding_seq(seq)\n",
    "        # e_struct = self.enc_embedding_struct(struct)\n",
    "        e_seq = self.enc_embedding(seq)\n",
    "        e_struct = self.enc_embedding(struct)\n",
    "        out_seq, hide_seq = self.enc_GRU_seq(e_seq)\n",
    "        out_struct, hide_struct = self.enc_GRU_struct(e_struct)\n",
    "\n",
    "        return out_seq, hide_seq, out_struct, hide_struct\n",
    "\n",
    "def Attention(hidden, key, mask=None, dropout=None): # Key and Value are the? same\n",
    "    # Goal: batch x embed x embed size\n",
    "    #print('attention', hidden.shape, key.shape)\n",
    "    # 1 x 512 * 512 x 512\n",
    "    scores = torch.bmm(hidden.permute(1, 0, 2), key.permute(0,2,1) ) \n",
    "    attention = F.softmax(scores, dim = -1)\n",
    "    # atn = 1 x 512, ie weights for each of the query\n",
    "    # Context vector\n",
    "    #print(attention.shape, hidden.permute(1, 0, 2).shape)\n",
    "    return torch.bmm(attention, key), attention\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(HybridAttention, self).__init__()\n",
    "        # START DCODE\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, enc_output, h_str, h_seq):\n",
    "        # Attention of each hidden state we look at and get context vetcor\n",
    "        d_str, a_str = Attention(h_str, enc_output)\n",
    "        d_seq, a_seq = Attention(h_seq, enc_output)\n",
    "        #print('contexts', d_str.shape, d_seq.shape)\n",
    "        # Get respective context vectors and combine together\n",
    "        # creats an aggregate contex vector.\n",
    "        d_t = self.linear1(torch.cat([d_str, d_seq], dim = -1))\n",
    "        #print('context vector', d_t.shape)\n",
    "        \n",
    "        return d_t\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, comm_vocab):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.dec_embedding = nn.Embedding(comm_vocab, hidden_size)\n",
    "        self.attention = HybridAttention(hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dec_GRU = nn.GRU(2*hidden_size, hidden_size, batch_first=True)\n",
    "        #self.dec_hid = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.pred = nn.Linear(hidden_size,  comm_vocab, 1) # Predict \n",
    "\n",
    "    def forward(self, encoder_outputs, h_seq, h_struct):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        \n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = h_seq\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            embed = self.dec_embedding(decoder_input)\n",
    "            #print('start', encoder_outputs.shape, h_seq.shape, h_struct.shape)\n",
    "            context = self.attention(encoder_outputs, h_seq, h_struct)\n",
    "\n",
    "            # what is supposedly an LSTM or something like that\n",
    "            #s_squiggly = self.tanh(self.dec_hid(torch.cat(embed, context)))\n",
    "            #output = self.pred(s_squiggly)\n",
    "            #print(embed.shape, context.shape)\n",
    "            output, decoder_hidden =  self.dec_GRU(torch.cat([embed, context], dim = -1), decoder_hidden)\n",
    "            output = self.pred(output)\n",
    "\n",
    "            _, topi = output.topk(1)\n",
    "            decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "            decoder_outputs.append(output) \n",
    "    \n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "     \n",
    "\n",
    "        return decoder_outputs#, decoder_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Construct Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, code_vocab, struct_vocab, comm_vocab):\n",
    "        super(Actor, self).__init__()\n",
    "        self.encoder = Encoder(input_size,hidden_size,code_vocab, struct_vocab)\n",
    "        self.decoder = AttnDecoder(hidden_size,comm_vocab)\n",
    "    def forward(self, code, struct):\n",
    "        out_seq, hide_seq, out_struct, hide_struct = self.encoder(code, struct)\n",
    "        \n",
    "        decoder_outputs = self.decoder(out_seq, hide_seq, hide_struct)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from paper\n",
    "batch_size = 64\n",
    "hidden_size = 512\n",
    "embedding_size = 512 #GRU, Layers\n",
    "lr = 0.001\n",
    "skip_num = 0\n",
    "code_vocab_size = len(vocab_code)\n",
    "struct_vocab_size = len(vocab_struct)\n",
    "comment_vocab_size = len(vocab_comm)\n",
    "#comment_max_len = max_comm # This and max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Critic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. Training Loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del ActorNetwork\n",
    "    del loss_actor_pretrain\n",
    "    del optim_actor\n",
    "except:\n",
    "    pass\n",
    "ActorNetwork = Actor(128, hidden_size, code_vocab_size, struct_vocab_size, comment_vocab_size).to(device)\n",
    "loss_actor_pretrain = nn.NLLLoss()\n",
    "optim_actor = optim.Adagrad(ActorNetwork.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m         out \u001b[38;5;241m=\u001b[39m ActorNetwork(seq_in, struct_in)\n\u001b[0;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_actor_pretrain(\n\u001b[0;32m     16\u001b[0m                 out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     17\u001b[0m                 target_tensor\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m         )\n\u001b[1;32m---> 19\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m out\n\u001b[0;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[0;32m     22\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: ActorNetwork\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     23\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optim_actor\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     24\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss},\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/actor_128_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\venv\\data\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\venv\\data\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\venv\\data\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torcheval.metrics import Perplexity\n",
    "try:\n",
    "        del out\n",
    "except:\n",
    "        pass\n",
    "for i in range(10):\n",
    "        print(i)\n",
    "        for train_in in train_dataloader:\n",
    "                seq_in = train_in[:,0,:]\n",
    "                struct_in =train_in[:,1,:]\n",
    "                target_tensor = train_in[:,2,:]\n",
    "                optim_actor.zero_grad()\n",
    "                \n",
    "                out = ActorNetwork(seq_in, struct_in)\n",
    "                loss = loss_actor_pretrain(\n",
    "                        out.view(-1, out.size(-1)),\n",
    "                        target_tensor.long().reshape(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                del out\n",
    "        torch.save({'epoch': i,\n",
    "           'model_state_dict': ActorNetwork.state_dict(),\n",
    "           'optimizer_state_dict': optim_actor.state_dict(),\n",
    "           'loss': loss},f'models/actor_128_{i}')\n",
    "        torch.save(ActorNetwork, f'models/actor_128_{i}.whole')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_sentence(indexes):\n",
    "    sentence = []\n",
    "    #print(indexes)\n",
    "    for index in indexes:\n",
    "        sentence.append(index_to_comm[index.item()])\n",
    "        if index == 1:\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict ['png/svg', '``\\\\\\\\d``', '__call__', 'vpc-6b1fe402', '跟踪', 'port_by_scheme``', 'pragma', '__option', 'nat', 'active\\r', 'commiss', 'device1', 'results\\r', 'source_dir', 'account', '``na', 'emphasi', 'get_credenti', 'new_axi', 'expected_arg', '7', 'one-vs-rest', 'client=', 'margin-bas', 'colormap=', 'drbd9', 'expiration_polici', 'x_3', 'x_3', '`examples/howto/patch_app', '`event`', 'journal={iclr}', 'auth_secret', 'tokenembedding`', 'sep``', '**kwarg', '--list-loc', 'branch/tag', 'new_valu', 'reflect', '/mydatacenter/vm/myvmfold', 'peform', 'batched_reduce_s', '在实盘中', 'non-http', 'encoding``', 'fog', '<config>', 'network_security_groups_list', 'ih', 'com/blog/2009/11/trying-out-retry-decorator-python/', 'wk2', 'get_zon', 'webserver/security/authentication/anonymousauthent', 'myapach', 'atla', '380', 'xml_path', 'name_map', '``msg``', 'rolling_window', 'pcmk_host_map=node1', 'tar/zip', 'servicegroup_server_up', 'mask', 'fielddescriptor', 'plot2', 'event_valu', 'autobuilding=tru', 'sessioncr', 'code_bodi', 'include_yes_decis', 'select_savenam', 'somepackag', 'targets}', 'rates/', 'satisfi', 'test', 'v6', '*directory*', 'errno=eio', 'minima', 'readonly=tru', 'host_creat', 'get_run', 'setopt', 'hardcod', '_`systemd', '``system', '`k-1`', 'redi', '保证金账户冻结的金额', '`resize_short`', '``__src_id``', 'unheld', 'textdata', 'till', 'stacker', 'dig', 'experience\\\\_in\\\\_month', 'cluster_uuid', 'up', '复权类型，', 'fade', 'settlement', 'image_input_nam', 'org/abs/1312', 'steps_per_epoch', 'profile`', 'install_args=<args>', '``show_changes``', '``time_created_local``', 'replace_node_statu', 'byte_item', 'falsey', 'getnumpartit', '``projects/*/locations/*``', 'nearestneighborclassifi', 'tent', '535', 'datetime/int', '``dist``', '**ssl', 'tout', 'seccertificateref', 'shell-lik', 'cert_requir', 'end=5']\n",
      "target ['<SOS>', 'get', 'oper', 'system', 'summari', 'train']\n"
     ]
    }
   ],
   "source": [
    "print('predict', out_to_sentence(decoded_ids[0]))\n",
    "print('target', out_to_sentence(target_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for test in test_dataloader:\n",
    "    seq_in = train_in[:,0,:]\n",
    "    struct_in =train_in[:,1,:]\n",
    "    target_tensor = train_in[:,2,:]\n",
    "    out = ActorNetwork(seq_in, struct_in)\n",
    "    _, topi = out.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
